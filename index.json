[{"content":"问题 问题： 在web服务开发中，http请求需要绑定到一个结构体上，如何处理结构体的零值问题？\n场景： 零值问题指无法确认是默认生成的零值还是用户传的零值，例如结构体A中有一个age字段为int类型，如果用户在请求时不传这个字段会默认反序列化为0，如果传了0也会是零。\n解决方案： 处理这类零值问题通常是将这个字段类型设置为对应的指针类型，通过指针是否为空判断调用方传没传这个值。\n缺陷： 但是validator无法校验指针类型的数据。\n如何灵活校验指针类型的数据？ 在model结构体中不直接进行校验，使用反射根据字符串调用函数进行校验\n ctx函数中（数据绑定，取非零值参数，通过字段名调用对应的校验函数）  // 数据绑定  if err := ctx.ShouldBindJSON(\u0026amp;data); err != nil { ginBindError(ctx, err, data) return } defer commonControllerDefer(ctx, \u0026amp;code, \u0026amp;msg, \u0026amp;data, \u0026amp;data) // 将结构体中非nil的字段提取到map中  if res, code = tools.StructToMap(data, \u0026#34;structs\u0026#34;); code != errmsg.SUCCESS { return } // 数据校验 将不同的字段绑定到不同的校验函数中，使用反射做校验  validateFunc := map[string]interface{}{ \u0026#34;name\u0026#34;: validator.Name, \u0026#34;phone\u0026#34;: validator.Phone, \u0026#34;work_experience\u0026#34;: validator.WorkExperience, \u0026#34;bio\u0026#34;: validator.Bio, \u0026#34;about\u0026#34;: validator.About, } // 反射数据校验  for key, value := range res { if msg, code = validator.CallFunc(validateFunc, key, value); code != errmsg.SUCCESS { return } } 通过反射提取非零值参数 tools.StructToMap  // StructToMap 结构体转为Map[string]interface{},忽略nil指针 func StructToMap(in interface{}, tagName string) (map[string]interface{}, int) { out := make(map[string]interface{}) v := reflect.ValueOf(in) if v.Kind() == reflect.Ptr { v = v.Elem() } if v.Kind() != reflect.Struct { // 非结构体返回错误提示  return nil, errmsg.ERROR } t := v.Type() // 遍历结构体字段  // 指定tagName值为map中key;字段值为map中value  for i := 0; i \u0026lt; v.NumField(); i++ { fi := t.Field(i) if tagValue := fi.Tag.Get(tagName); tagValue != \u0026#34;\u0026#34; { // 如果这个指向的是一个空指针就不用添加到map里去。  if !v.Field(i).IsNil() { out[tagValue] = v.Field(i).Interface() } } } return out, errmsg.SUCCESS } 通过字符串调用对应的函数validator.CallFunc(validateFunc, key, value)  func CallFunc(m map[string]interface{}, name string, params ...interface{}) (string, int) { defer func() { if err := recover(); err != nil { fmt.Println(err) logger.Log.Error(\u0026#34;反射校验字段panic\u0026#34;, zap.String(\u0026#34;errorMsg\u0026#34;, fmt.Sprintf(\u0026#34;%v\u0026#34;, err))) } }() if m[name] == nil { return fmt.Sprintf(\u0026#34;不存在字段%s校验函数\u0026#34;, name), errmsg.ErrorInput } f := reflect.ValueOf(m[name]) in := make([]reflect.Value, len(params)) for k, param := range params { in[k] = reflect.ValueOf(param) } result := f.Call(in) return result[0].String(), int(result[1].Int()) } 完整的代码可以在github的仓库中找到https://github.com/shiluanzzz/Advisor_service/blob/master/utils/validator/commonValidator.go\n参考链接 ","permalink":"http://www.shiluan.space/post/2208/golang%E9%80%9A%E8%BF%87%E5%8F%8D%E5%B0%84%E6%A0%A1%E9%AA%8C%E7%BB%93%E6%9E%84%E4%BD%93%E4%B8%AD%E7%9A%84%E9%9B%B6%E5%80%BC%E5%AD%97%E6%AE%B5/","summary":"问题 问题： 在web服务开发中，http请求需要绑定到一个结构体上，如何处理结构体的零值问题？\n场景： 零值问题指无法确认是默认生成的零值还是用户传的零值，例如结构体A中有一个age字段为int类型，如果用户在请求时不传这个字段会默认反序列化为0，如果传了0也会是零。\n解决方案： 处理这类零值问题通常是将这个字段类型设置为对应的指针类型，通过指针是否为空判断调用方传没传这个值。\n缺陷： 但是validator无法校验指针类型的数据。\n如何灵活校验指针类型的数据？ 在model结构体中不直接进行校验，使用反射根据字符串调用函数进行校验\n ctx函数中（数据绑定，取非零值参数，通过字段名调用对应的校验函数）  // 数据绑定  if err := ctx.ShouldBindJSON(\u0026amp;data); err != nil { ginBindError(ctx, err, data) return } defer commonControllerDefer(ctx, \u0026amp;code, \u0026amp;msg, \u0026amp;data, \u0026amp;data) // 将结构体中非nil的字段提取到map中  if res, code = tools.StructToMap(data, \u0026#34;structs\u0026#34;); code != errmsg.SUCCESS { return } // 数据校验 将不同的字段绑定到不同的校验函数中，使用反射做校验  validateFunc := map[string]interface{}{ \u0026#34;name\u0026#34;: validator.Name, \u0026#34;phone\u0026#34;: validator.Phone, \u0026#34;work_experience\u0026#34;: validator.WorkExperience, \u0026#34;bio\u0026#34;: validator.Bio, \u0026#34;about\u0026#34;: validator.About, } // 反射数据校验  for key, value := range res { if msg, code = validator.","title":"Golang通过反射校验结构体中的零值字段"},{"content":"创建表格 create table userinfo( id int not null AUTO_INCREMENT, name varchar(100) not null, lasttime int , primary key (id) );   创建远程访问的用户\n  创建用户  create user test identified by '123456';\n  创建数据表 create table_name;\n  授权 gran all privileges on ``table_name``.* to Luke@'%' identified by 'ssss6666';\n    删除unique key\n alter table table_name drop index index_name;    新增unique key\n alter table table_name add unique key new_index_name (col1,col2);    查看表的结构\n  desc table_name显示表结构，字段类型，主键是否为空等。但是不显示外键\n  show create table table_name 显示创建表的DDL\n    修改表的储存引擎ALTER TABLE account ENGINE=MyISAM;\n  修改表 ALTER TABLE   修改表名 ALTER TABLE \u0026lt;表名\u0026gt; RENAME \u0026lt;新表名\u0026gt;\n 例:ALTER TABLE game_account RENAME account;    添加字段ALTER TABLE \u0026lt;表名\u0026gt; ADD \u0026lt;字段名称\u0026gt; \u0026lt;字段定义\u0026gt; [ FIRST | AFTER col_name]\nALTER TABLE account ADD Game_zone VARCHAR(20) NOT NULL DEFAULT \u0026quot;HuaBei\u0026quot; FIRST;\n 如果需要指定加在某个位置，在语句最后加上FIRST 或者 AFTER col_name    删除字段ALTER TABLE \u0026lt;表名\u0026gt; DROP \u0026lt;字段名称\u0026gt;\nALTER TABLE account DROP Account_role ;\n  修改字段ALTER TABLE \u0026lt;表名\u0026gt; CHANGE \u0026lt;旧字段名称\u0026gt; \u0026lt;新字段名称\u0026gt; \u0026lt;字段定义\u0026gt;\nALTER TABLE account CHANGE password password VARCHAR(60) ;\n  修改表的编码 ALTER TABLE \u0026lt;表名\u0026gt; character set utf8mb4;\nalter table email character set utf8mb4;\n  修改数据库的编码 ALTER database \u0026lt;库名\u0026gt; character set utf8mb4;\n  导出|导入 导出数据\n  导出整个数据库 mysqldump -u 用户名 -p 数据库名 \u0026gt; 导出的文件名\n  导出表mysqldump -u 用户名 -p 数据库名 表名\u0026gt; 导出的文件名\n 使用mysqldump 导出数据库可能会产生中文乱码的情况，加上--default-character-set=utf8 即可解决导出乱码问题    上面两条命令都是带数据的导出，如果不带数据导出，在-p 添加 -d --add-drop-table\n  导入数据\n  进入mysql控制台后使用source命令 source xxx.sql\n  ~~使用cmd执行 ~~~~~~\n  导出查询结果\n  编辑好一个sql查询文件, 例如文件名 user_id.sql,内容如下\nuse psychic; select distinct user_id from `order`;   mysql -uUserName -pPasssword \u0026lt; user.sql \u0026gt;res.txt\n  存储时间字段   varchar: 存放yyyy-MM-dd HH:mm:ss 的时间格式字符串\n  datetime: 存放与上面格式一致的时间，与varchar的区别在于，它对时间做了校检，可以避免写入2月31日这样的无效日期\n  timestamp：存放时间戳，默认是10位数的（timestamp(0))，精确到秒。然后如果想存储毫秒，纳秒需要把长度对应地加大。毫秒13位长度，timestamp(3)，更精细的timestamp(6)\n  字段长度问题   整数类型：显示宽度和数据的取值范围没有关系，如果超过了显示宽度依然可以正常插入。\n不设置宽度，系统将添加默认的宽度 tinyint(4),smallint(6),mediumint(9),int(11),bigint(20)\n  字符串类型：varchar(20) 指的是字符串的最大长度，如果输入了一个21长度的字符串则会丢失信息。由于varchar是变成存储的，所以开发过程中通常设置宽度为255，不用完也不会浪费空间。\n **varchar变长存储相关 **https://liuchenyang0515.blog.csdn.net/article/details/117524328     浮点和日期等数据类型对数据的宽度没有要求一般也不用设定。\n  开发总结   create_at字段，设置默认值为CURRENT_TIMESTAMP\n  update_at字段设置为DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP，当条目被修改时会自动更新\n  is_del字段，用来表示是否被删除- -\n  id 一般都用自增id作为主键\n  尽量避免关联查询\n  utf8mb4是utf8的超集，理论上由utf8升级到utf8mb4字符编码没有任何兼容问题。utf8mb4主要是解决了MySQL数据库存储emoji表情的问题。\n  blob,text的区别以及性能影响\nhttps://www.cnblogs.com/wt645631686/p/10102509.html\nBLOB字段用于存储二进制数据，是一个可以存储大量数据的容器，它能容纳不同大小的数据。\n类型 大小(单位：字节) TinyBlob 最大 255 Blob 最大 65K MediumBlob 最大 16M LongBlob 最大 4G\n  mysql下如何查看字段的长度\nlength()： 单位是字节，utf8编码下,一个汉字三个字节，一个数字或字母一个字节。gbk编码下,一个汉字两个字节，一个数字或字母一个字节。\nhttps://www.py.cn/db/mysql/20100.html\n  mysql中text,tinytext,varchar的区别。varchar的长度限制\n 如果是海量文本的存储，可以直接用mongodb做kv存储。    MySQL事务\n事务内容\n  查询订单状态是否为1\n  修改订单状态为2\n  问题：如果事务在查询订单状态为1后，在修改钱订单状态变为了其他，这个事务最终还能提交成功吗？\nA: 事务一般只做修改，查询的话不行\n  bug排查记录 https://blog.csdn.net/jlu16/article/details/82809937 https://blog.csdn.net/github_38336924/article/details/82455500\n参考链接 https://learnku.com/articles/46979 https://github.com/didi/gendry/issues https://www.runoob.com/mysql\n","permalink":"http://www.shiluan.space/post/2208/mysql%E7%BB%88%E7%AB%AF%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93/","summary":"创建表格 create table userinfo( id int not null AUTO_INCREMENT, name varchar(100) not null, lasttime int , primary key (id) );   创建远程访问的用户\n  创建用户  create user test identified by '123456';\n  创建数据表 create table_name;\n  授权 gran all privileges on ``table_name``.* to Luke@'%' identified by 'ssss6666';\n    删除unique key\n alter table table_name drop index index_name;    新增unique key\n alter table table_name add unique key new_index_name (col1,col2);    查看表的结构","title":"MySQL终端命令总结"},{"content":"什么是JWT  Json web token (JWT), 是为了在网络应用环境间传递声明而执行的一种基于JSON的开放标准（(RFC 7519). 该token被设计为紧凑且安全的，特别适用于分布式站点的单点登录（SSO）场景。JWT的声明一般被用来在身份提供者和服务提供者间传递被认证的用户身份信息，以便于从资源服务器获取资源，也可以增加一些额外的其它业务逻辑所必须的声明信息，该token也可直接被用于认证，也可被加密。\n 传统的session认证 http协议本身是一种无状态的协议，而这就意味着如果用户向我们的应用提供了用户名和密码来进行用户认证，那么下一次请求时，用户还要再一次进行用户认证才行，因为根据http协议，我们并不能知道是哪个用户发出的请求，所以为了让我们的应用能识别是哪个用户发出的请求，我们只能在服务器存储一份用户登录的信息，这份登录信息会在响应时传递给浏览器，告诉其保存为cookie,以便下次请求时发送给我们的应用，这样我们的应用就能识别请求来自哪个用户了,这就是传统的基于session认证。\n但是这种基于session的认证使应用本身很难得到扩展，随着不同客户端用户的增加，独立的服务器已无法承载更多的用户，而这时候基于session认证应用的问题就会暴露出来.\n问题：\n Session: 每个用户经过我们的应用认证之后，我们的应用都要在服务端做一次记录，以方便用户下次请求的鉴别，通常而言session都是保存在内存中，而随着认证用户的增多，服务端的开销会明显增大。 **扩展性: ** 用户认证之后，服务端做认证记录，如果认证的记录被保存在内存中的话，这意味着用户下次请求还必须要请求在这台服务器上,这样才能拿到授权的资源，这样在分布式的应用上，相应的限制了负载均衡器的能力。这也意味着限制了应用的扩展能力。 **CSRF: **因为是基于cookie来进行用户识别的, cookie如果被截获，用户就会很容易受到跨站请求伪造的攻击。  基于token的鉴权机制 基于token的鉴权机制类似于http协议也是无状态的，但是它不需要在服务端去保留用户的认证信息或者会话信息。鉴权机制大致流程如下\n 用户使用用户名密码来请求服务器 服务器验证用户的信息 服务器通过验证发送给用户一个token 客户端存储token，并在每次请求时附送上这个token值 服务端验证token值，并返回数据  JWT的构成 jwt主要由三部分组成（header,payload,signature），分别为头部、载荷和签证。每个部分之间用.隔开。\nheader  jwt的头部主要记录声明类型和加密算法，完整的头部如下面的json所示。\n{ \u0026#39;typ\u0026#39;: \u0026#39;JWT\u0026#39;, \u0026#39;alg\u0026#39;: \u0026#39;HS256\u0026#39; } 对头部进行base64加密后即构成jwt的第一部分，例如上面的json经过base64加密后转化为eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9\nplayload  载荷存放token的声明，包含标准中注册的声明、公共的声明和私有的声明。\n标准中注册的声明 (建议但不强制使用) ：\niss: jwt签发者 sub: jwt所面向的用户 aud: 接收jwt的一方 exp: jwt的过期时间，这个过期时间必须要大于签发时间 nbf: 定义在什么时间之前，该jwt都是不可用的. iat: jwt的签发时间 jti: jwt的唯一身份标识，主要用来作为一次性token,从而回避重放攻击。 公共的声明 ：公共的声明可以添加任何的信息，一般添加用户的相关信息或其他业务需要的必要信息.但不建议添加敏感信息，因为该部分在客户端可解密.\n私有的声明 ：私有声明是提供者和消费者所共同定义的声明，一般不建议存放敏感信息，因为base64是对称解密的，意味着该部分信息可以归类为明文信息。\nsignature  signature即jwt的签证信息，该部分的生成需要前面提到的header (base64后的)，payload (base64后的), secret。\n首先将header和payload通过.连接组成字符串，对对这个字符串进行加盐加密构成secret，即jwt的第三部分。有关加盐存储可以看后端密码储存方案。\n// javascript var encodedString = base64UrlEncode(header) + \u0026#39;.\u0026#39; + base64UrlEncode(payload); var signature = HMACSHA256(encodedString, \u0026#39;secret\u0026#39;); // TJVA95OrM7E2cBab30RMHrHDcEfxjoYZgeFONFh7HgQ 将这三部分用.连接成一个完整的字符串,构成了最终的jwt。\n需要注意的是密钥的保存、签发生成都是在服务端生成的。而secret就是服务端的私钥，在任何情况下都不应该泄露出去，如果泄露会被伪造jwt。\njwt的总结 优点  因为json的通用性，所以JWT是可以进行跨语言支持的，像JAVA,JavaScript,NodeJS,PHP等很多语言都可以使用。 因为有了payload部分，所以JWT可以在自身存储一些其他业务逻辑所必要的非敏感信息。 便于传输，jwt的构成非常简单，字节占用很小，所以它是非常便于传输的。 它不需要在服务端保存会话信息, 所以它易于应用的扩展  安全相关  不应该在jwt的payload部分存放敏感信息，因为该部分是客户端可解密的部分。 保护好secret私钥，该私钥非常重要。 如果可以，请使用https协议  关于jwt的问题  在退出登录or修改密码的时候，如何实现token 失效？  在服务端存一个token的黑名单，失效就加入黑名单中。 在服务端设置加密的 key 时，为每个用户生成唯一的 key，失效则改变该 key 在playload部分增加一个版本号字段，失效就更改该版本号。    ","permalink":"http://www.shiluan.space/post/2208/jwt/","summary":"什么是JWT  Json web token (JWT), 是为了在网络应用环境间传递声明而执行的一种基于JSON的开放标准（(RFC 7519). 该token被设计为紧凑且安全的，特别适用于分布式站点的单点登录（SSO）场景。JWT的声明一般被用来在身份提供者和服务提供者间传递被认证的用户身份信息，以便于从资源服务器获取资源，也可以增加一些额外的其它业务逻辑所必须的声明信息，该token也可直接被用于认证，也可被加密。\n 传统的session认证 http协议本身是一种无状态的协议，而这就意味着如果用户向我们的应用提供了用户名和密码来进行用户认证，那么下一次请求时，用户还要再一次进行用户认证才行，因为根据http协议，我们并不能知道是哪个用户发出的请求，所以为了让我们的应用能识别是哪个用户发出的请求，我们只能在服务器存储一份用户登录的信息，这份登录信息会在响应时传递给浏览器，告诉其保存为cookie,以便下次请求时发送给我们的应用，这样我们的应用就能识别请求来自哪个用户了,这就是传统的基于session认证。\n但是这种基于session的认证使应用本身很难得到扩展，随着不同客户端用户的增加，独立的服务器已无法承载更多的用户，而这时候基于session认证应用的问题就会暴露出来.\n问题：\n Session: 每个用户经过我们的应用认证之后，我们的应用都要在服务端做一次记录，以方便用户下次请求的鉴别，通常而言session都是保存在内存中，而随着认证用户的增多，服务端的开销会明显增大。 **扩展性: ** 用户认证之后，服务端做认证记录，如果认证的记录被保存在内存中的话，这意味着用户下次请求还必须要请求在这台服务器上,这样才能拿到授权的资源，这样在分布式的应用上，相应的限制了负载均衡器的能力。这也意味着限制了应用的扩展能力。 **CSRF: **因为是基于cookie来进行用户识别的, cookie如果被截获，用户就会很容易受到跨站请求伪造的攻击。  基于token的鉴权机制 基于token的鉴权机制类似于http协议也是无状态的，但是它不需要在服务端去保留用户的认证信息或者会话信息。鉴权机制大致流程如下\n 用户使用用户名密码来请求服务器 服务器验证用户的信息 服务器通过验证发送给用户一个token 客户端存储token，并在每次请求时附送上这个token值 服务端验证token值，并返回数据  JWT的构成 jwt主要由三部分组成（header,payload,signature），分别为头部、载荷和签证。每个部分之间用.隔开。\nheader  jwt的头部主要记录声明类型和加密算法，完整的头部如下面的json所示。\n{ \u0026#39;typ\u0026#39;: \u0026#39;JWT\u0026#39;, \u0026#39;alg\u0026#39;: \u0026#39;HS256\u0026#39; } 对头部进行base64加密后即构成jwt的第一部分，例如上面的json经过base64加密后转化为eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9\nplayload  载荷存放token的声明，包含标准中注册的声明、公共的声明和私有的声明。\n标准中注册的声明 (建议但不强制使用) ：\niss: jwt签发者 sub: jwt所面向的用户 aud: 接收jwt的一方 exp: jwt的过期时间，这个过期时间必须要大于签发时间 nbf: 定义在什么时间之前，该jwt都是不可用的. iat: jwt的签发时间 jti: jwt的唯一身份标识，主要用来作为一次性token,从而回避重放攻击。 公共的声明 ：公共的声明可以添加任何的信息，一般添加用户的相关信息或其他业务需要的必要信息.但不建议添加敏感信息，因为该部分在客户端可解密.\n私有的声明 ：私有声明是提供者和消费者所共同定义的声明，一般不建议存放敏感信息，因为base64是对称解密的，意味着该部分信息可以归类为明文信息。\nsignature  signature即jwt的签证信息，该部分的生成需要前面提到的header (base64后的)，payload (base64后的), secret。","title":"Json Web Token"},{"content":"日志Trace-Id 在后端服务中使用logrus作为日志系统，当接受到一条请求时，该请求可能会调用其他层(controller,biz,service)的代码而产生很多日志，这时候就需要一个trace-id来快速定位到与某一条请求相关的日志。\n如果是在每条日志前手动加一个id，那也太蠢了。主要实现方法有两种\n 使用context在每个函数之前传递id，这样实际上还是多了一个参数，不够优雅 使用webhook为当前作用域添加一个field  使用webhook为日志添加Trace-Id 自定义了一个TraceId的结构体，实现了logrus中的hook接口，该接口需要实现Fire和Levels两个方法\n// 自定义一个TraceIdHook 并实现Fire和Levels方法 type TraceIdHook struct { TraceId string } func NewTraceIdHook(traceId string) logrus.Hook { hook := TraceIdHook{ TraceId: traceId, } return \u0026amp;hook } func (hook *TraceIdHook) Fire(entry *logrus.Entry) error { // 注入trace-ID  entry.Data[\u0026#34;trace-ID\u0026#34;] = hook.TraceId return nil } func (hook *TraceIdHook) Levels() []logrus.Level { return logrus.AllLevels } 模拟不同作用域下的使用,完整的代码可以在go-playground看到\nfunc doSth(params string) { logrus.Info(\u0026#34;do sth\u0026#34;, params) subJob(params) } func subJob(params string) { logrus.Info(\u0026#34;sub job\u0026#34;, params) } func main() { { logrus.AddHook(NewTraceIdHook(\u0026#34;goroutine1\u0026#34;)) doSth(\u0026#34;job1\u0026#34;) } { logrus.AddHook(NewTraceIdHook(\u0026#34;goroutine2\u0026#34;)) doSth(\u0026#34;job2\u0026#34;) } time.Sleep(1 * time.Second) } output\n{\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;do sthjob1\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2022-06-27T16:29:57+08:00\u0026#34;,\u0026#34;trace-ID\u0026#34;:\u0026#34;goroutine1\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;sub jobjob1\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2022-06-27T16:29:57+08:00\u0026#34;,\u0026#34;trace-ID\u0026#34;:\u0026#34;goroutine1\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;do sthjob2\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2022-06-27T16:29:57+08:00\u0026#34;,\u0026#34;trace-ID\u0026#34;:\u0026#34;goroutine2\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;sub jobjob2\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2022-06-27T16:29:57+08:00\u0026#34;,\u0026#34;trace-ID\u0026#34;:\u0026#34;goroutine2\u0026#34;} 添加到后端服务中 以gin框架为基础的后端服务中，我们只需要在controller层调用前logrus.AddHook(NewTraceIdHook(\u0026quot;your id \u0026quot;))即可。\n我的实践是自定义的context中调用这个函数,当然用其他方法也可。\nfunc extendContext(fn func(*lib.MyContext) error) gin.HandlerFunc { return func(context *gin.Context) { ctx := \u0026amp;lib.MyContext{ Context: context, } ... logrus.AddHook(logger.NewTraceIdHook(uuid.New().String())) fn(ctx) } } 最终的效果如下 参考链接：  https://www.cnblogs.com/lgh344902118/p/15339016.html  ","permalink":"http://www.shiluan.space/post/logrus%E4%B8%BAhttp%E8%AF%B7%E6%B1%82%E8%87%AA%E5%8A%A8%E6%B7%BB%E5%8A%A0trace-id/","summary":"日志Trace-Id 在后端服务中使用logrus作为日志系统，当接受到一条请求时，该请求可能会调用其他层(controller,biz,service)的代码而产生很多日志，这时候就需要一个trace-id来快速定位到与某一条请求相关的日志。\n如果是在每条日志前手动加一个id，那也太蠢了。主要实现方法有两种\n 使用context在每个函数之前传递id，这样实际上还是多了一个参数，不够优雅 使用webhook为当前作用域添加一个field  使用webhook为日志添加Trace-Id 自定义了一个TraceId的结构体，实现了logrus中的hook接口，该接口需要实现Fire和Levels两个方法\n// 自定义一个TraceIdHook 并实现Fire和Levels方法 type TraceIdHook struct { TraceId string } func NewTraceIdHook(traceId string) logrus.Hook { hook := TraceIdHook{ TraceId: traceId, } return \u0026amp;hook } func (hook *TraceIdHook) Fire(entry *logrus.Entry) error { // 注入trace-ID  entry.Data[\u0026#34;trace-ID\u0026#34;] = hook.TraceId return nil } func (hook *TraceIdHook) Levels() []logrus.Level { return logrus.AllLevels } 模拟不同作用域下的使用,完整的代码可以在go-playground看到\nfunc doSth(params string) { logrus.Info(\u0026#34;do sth\u0026#34;, params) subJob(params) } func subJob(params string) { logrus.","title":"Logrus为http请求自动添加trace Id"},{"content":"对操作系统来说，线程是最小的执行单元，进程是最小的资源管理单元。 无论进程还是线程，都是由操作系统所管理的。协程(Coroutines)是一种比线程更加轻量级的存在\n1进程 一个进程好比是一个程序，它是资源分配的最小单位 。也是CPU的最小工作单元，就是说操作系统同一时刻执行的进程数不会超过核心数。\n电脑中有许多进程需要处于「同时」开启的状态，而利用CPU在进程间的快速切换，可以实现「同时」运行多个程序。而进程切换则意味着需要保留进程切换前的状态，以备切换回去的时候能够继续接着工作。 所以进程拥有自己的地址空间，全局变量，文件描述符，各种硬件等等资源。操作系统通过调度CPU去执行进程的记录、回复、切换等等。\n2线程 线程是一个程序执行过程中的最小单元，一个程序可能包含多个线程，这多个线程共享进程的所有资源。每个线程执行不同的任务来提高程序的运行效率。\n3进程与线程的区别  进程是CPU资源分配的基本单位，线程是独立运行和独立调度的基本单位（CPU上真正运行的是线程）。 进程拥有自己的资源空间，一个进程包含若干个线程，线程与CPU资源分配无关，多个线程共享同一进程内的资源。 线程的调度与切换比进程快很多。  4为什么要有线程？ 线程的出现就是为了提高单位时间内CPU的利用率。\n任务一般分为两种：CPU密集型和IO密集型。\n CPU密集型任务对CPU的利用率较高 IO密集型的程序因为包含了大量的IO操作，所以其状态在就绪、运行、阻塞之前频繁切换。  随着程序的功能越来越复杂，程序更加偏向于IO密集型，而IO密集型任务也就是无法即使完成任务而带来大量的上下文切换。而进程之间上下文切换的代价是比较高的。\n因此为了使用更小的粒度提高进程单位时间的CPU利用率，就有了线程的概念。进程作为线程的容器，可以按不同的功能，或想达到更高效率（如多个IO线程）的同一功能，可以考虑产生多个线程。\n因为在CPU切换到本进程的时间段时，由于线程间共享进程的上下文，线程切换只需要切换线程的上下文，而不需要切换另一片内存或者寄存器资源，在功能并行执行的同时降低了开销。\n但是需要注意的是开更多的线程不会导致本进程得到更多CPU的青睐!多线程只能提高进程在执行过程中的CPU利用率。\n5什么是协程？ 虽然线程大幅的提高了CPU的效率，且能够设置一定的优先级，但是线程的资源片分配还是由CPU来管理的。\n那么能不能人为管理线程的资源分配（切换）呢？协程在语言层面实现了这一点。\n如同一个进程可以有很多线程一样，一个线程可以有很多协程。\n但是，协程不是被操作系统所管理的，没有改变CPU最小执行单元是线程，协程是完全由程序所控制的（用户态执行），不会产生上下文切换。\n目前只有部分语言实现了协程：\n python的yield/send，当协程执行到yield关键字时，会暂停在那一行，等到主线程调用send方法发送了数据，协程才会接到数据继续执行。 Lua从5.0版本开始使用协程，通过扩展库coroutine来实现。 Go语言对协程的实现非常强大而简洁，可以轻松创建成百上千个协程并发执行。 Java语言并没有对协程的原生支持，但是某些开源框架模拟出了协程的功能，可以看一看Kilim框架的源码：https://github.com/kilim/kilim  参考链接：  https://zhuanlan.zhihu.com/p/70256971  ","permalink":"http://www.shiluan.space/post/%E7%BA%BF%E7%A8%8B%E8%BF%9B%E7%A8%8B%E5%8D%8F%E7%A8%8B/","summary":"对操作系统来说，线程是最小的执行单元，进程是最小的资源管理单元。 无论进程还是线程，都是由操作系统所管理的。协程(Coroutines)是一种比线程更加轻量级的存在\n1进程 一个进程好比是一个程序，它是资源分配的最小单位 。也是CPU的最小工作单元，就是说操作系统同一时刻执行的进程数不会超过核心数。\n电脑中有许多进程需要处于「同时」开启的状态，而利用CPU在进程间的快速切换，可以实现「同时」运行多个程序。而进程切换则意味着需要保留进程切换前的状态，以备切换回去的时候能够继续接着工作。 所以进程拥有自己的地址空间，全局变量，文件描述符，各种硬件等等资源。操作系统通过调度CPU去执行进程的记录、回复、切换等等。\n2线程 线程是一个程序执行过程中的最小单元，一个程序可能包含多个线程，这多个线程共享进程的所有资源。每个线程执行不同的任务来提高程序的运行效率。\n3进程与线程的区别  进程是CPU资源分配的基本单位，线程是独立运行和独立调度的基本单位（CPU上真正运行的是线程）。 进程拥有自己的资源空间，一个进程包含若干个线程，线程与CPU资源分配无关，多个线程共享同一进程内的资源。 线程的调度与切换比进程快很多。  4为什么要有线程？ 线程的出现就是为了提高单位时间内CPU的利用率。\n任务一般分为两种：CPU密集型和IO密集型。\n CPU密集型任务对CPU的利用率较高 IO密集型的程序因为包含了大量的IO操作，所以其状态在就绪、运行、阻塞之前频繁切换。  随着程序的功能越来越复杂，程序更加偏向于IO密集型，而IO密集型任务也就是无法即使完成任务而带来大量的上下文切换。而进程之间上下文切换的代价是比较高的。\n因此为了使用更小的粒度提高进程单位时间的CPU利用率，就有了线程的概念。进程作为线程的容器，可以按不同的功能，或想达到更高效率（如多个IO线程）的同一功能，可以考虑产生多个线程。\n因为在CPU切换到本进程的时间段时，由于线程间共享进程的上下文，线程切换只需要切换线程的上下文，而不需要切换另一片内存或者寄存器资源，在功能并行执行的同时降低了开销。\n但是需要注意的是开更多的线程不会导致本进程得到更多CPU的青睐!多线程只能提高进程在执行过程中的CPU利用率。\n5什么是协程？ 虽然线程大幅的提高了CPU的效率，且能够设置一定的优先级，但是线程的资源片分配还是由CPU来管理的。\n那么能不能人为管理线程的资源分配（切换）呢？协程在语言层面实现了这一点。\n如同一个进程可以有很多线程一样，一个线程可以有很多协程。\n但是，协程不是被操作系统所管理的，没有改变CPU最小执行单元是线程，协程是完全由程序所控制的（用户态执行），不会产生上下文切换。\n目前只有部分语言实现了协程：\n python的yield/send，当协程执行到yield关键字时，会暂停在那一行，等到主线程调用send方法发送了数据，协程才会接到数据继续执行。 Lua从5.0版本开始使用协程，通过扩展库coroutine来实现。 Go语言对协程的实现非常强大而简洁，可以轻松创建成百上千个协程并发执行。 Java语言并没有对协程的原生支持，但是某些开源框架模拟出了协程的功能，可以看一看Kilim框架的源码：https://github.com/kilim/kilim  参考链接：  https://zhuanlan.zhihu.com/p/70256971  ","title":"线程、进程、协程的区别"},{"content":"Hash的思想 Hash 的核心思想在于，将输入映射到一个值域较小、可以方便比较的范围。\n我们定义一个把字符串映射到整数的函数 ，这个$f$称为是 Hash 函数。这个函数$f$可以方便地帮我们判断两个字符串是否相等。\n 哈希值不一样的字符串一定不一样 哈希值一样的字符串不一定一样（可能出现哈希碰撞的情况）  字符串哈希 通常采用多项式hash方法，即对于一个长度为$l$的字符串，定义哈希函数为\n$$ f(s)=\\sum_{i=1}^{l} s[i] \\times b^{l-i}(\\bmod M) $$\n例如，字符串$xyz$的哈希值为$xb^2+yb+z$。\nHash的实现 # Python Version M = int(1e9 + 7) B = 233 def get_hash(s): res = 0 for char in s: res = (res * B + ord(char)) % M return res def cmp(s, t): return get_hash(s) == get_hash(t)   错误率分析\n若进行 $n$ 次比较，每次错误率 $\\dfrac 1 M$，那么总错误率是 $1-\\left(1-\\dfrac 1 M\\right)^n$。在随机数据下，若 $M=10^9 + 7$，$n=10^6$，错误率约为 $\\dfrac 1{1000}$，并不是能够完全忽略不计的。\n  改进| 多次询问子串哈希 单次计算一个字符串的哈希时间复杂度为$O(n)$，n为串长。如果多次询问一个字符串的子串的哈希值，每次重新计算效率低下。\n一般采取的方法是对整个字符串先预处理出每个前缀的哈希值，将哈希值看成一个 $b$ 进制的数对 $M$ 取模的结果，这样的话每次就能快速求出子串的哈希了：\n令 $f_i(s)$ 表示 $f(s[1..i])$，即原串长度为 $i$ 的前缀的哈希值，那么按照定义有\n$$ f_i(s)=s[1]\\cdot b^{i-1}+s[2]\\cdot b^{i-2}+\u0026hellip;+s[i-1]\\cdot b+s[i] $$\n现在，我们想要用类似前缀和的方式快速求出 $f(s[l..r])$，按照定义有字符串 $s[l..r]$ 的哈希值为\n$$ f(s[l..r])=s[l]\\cdot b^{r-l}+s[l+1]\\cdot b^{r-l-1}+\u0026hellip;+s[r-1]\\cdot b+s[r] $$\n对比观察上述两个式子，我们发现 $f(s[l..r])=f_r(s)-f_{l-1}(s) \\times b^{r-l+1}$ 成立（可以手动代入验证一下），因此我们用这个式子就可以快速得到子串的哈希值。其中 $b^{r-l+1}$ 可以 $O(n)$ 的预处理出来然后 $O(1)$ 的回答每次询问（当然也可以快速幂 $O(\\log n)$ 的回答每次询问）。\nRabin-Karp 字符串编码  RK字符串编码的思想跟上面多次询问子串哈希一样，以一个仅包含小写英文字母的字符串编码为例。\n 字符串中仅包含小写字母，可以使用arr[i]=ord('i')-ord('a')将字母编码为0-25之间的数字，比如字符串abcde可编码为[0,1,2,3,4]。 将子串看成一个26进制的数，这个数对应的10进制数就是它的编码。  字符串abc的编码为$h_0=0 \\times 26^2 + 1 \\times 26^1 + 2 \\times 26^0 = 28$ 公式表达：对于一个长度为$L$的字符串，设$c_i$为 $s$ 的第 $i$ 个字符编码后的数字，$a$ 为编码的进制，那么有 $h_0 = c_0a^{L-1} + c_1a^{L-2} + \u0026hellip; +c_{L-1}a^0 = \\sum_{i=0}^{L-1} c_ia^{L-1-i}$   在上一步中，对每一个子串的求解编码时间复杂度仍然为$O(n)$，但是当求同一个长度的字符串编码时，可以利用相邻的同长度字符串来求解。   字符串abc的编码为$h_0=0 \\times 26^2 + 1 \\times 26^1 + 2 \\times 26^0 = 28$\n  字符串bcd的编码$h_1=1 \\times 26^2 + 2 \\times 26^1 + 3 \\times 26^0 = 731 $，其实h1可以在h0的基础上，更快的求解出编码，即$h_1=(h_0-0\\times26^2)\\times26+3\\times26^0=731 $\n更一般的表达式为：$h_{1}=\\left(h_{0} \\times a-c_{0} \\times a^{L}\\right)+c_{L+1} $\n  这样即可在常数时间内根据上一个子串求解出下一个子串的字符串编码。\n   当字符个数过多后，字符串编码的数会特别大，一般的做法是需要对编码进行取模来防止溢出，模一般选取编码的信息量的平方的数量级。而取模则会带来哈希碰撞。  为了避免哈希碰撞可以使用两套进制和模的组合。当两套都一样时视作相同。 或者对编码在调用一次内置的hash函数进行哈希。    ","permalink":"http://www.shiluan.space/post/%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%93%88%E5%B8%8C/","summary":"Hash的思想 Hash 的核心思想在于，将输入映射到一个值域较小、可以方便比较的范围。\n我们定义一个把字符串映射到整数的函数 ，这个$f$称为是 Hash 函数。这个函数$f$可以方便地帮我们判断两个字符串是否相等。\n 哈希值不一样的字符串一定不一样 哈希值一样的字符串不一定一样（可能出现哈希碰撞的情况）  字符串哈希 通常采用多项式hash方法，即对于一个长度为$l$的字符串，定义哈希函数为\n$$ f(s)=\\sum_{i=1}^{l} s[i] \\times b^{l-i}(\\bmod M) $$\n例如，字符串$xyz$的哈希值为$xb^2+yb+z$。\nHash的实现 # Python Version M = int(1e9 + 7) B = 233 def get_hash(s): res = 0 for char in s: res = (res * B + ord(char)) % M return res def cmp(s, t): return get_hash(s) == get_hash(t)   错误率分析\n若进行 $n$ 次比较，每次错误率 $\\dfrac 1 M$，那么总错误率是 $1-\\left(1-\\dfrac 1 M\\right)^n$。在随机数据下，若 $M=10^9 + 7$，$n=10^6$，错误率约为 $\\dfrac 1{1000}$，并不是能够完全忽略不计的。","title":"字符串哈希"},{"content":"Hash的思想 Hash 的核心思想在于，将输入映射到一个值域较小、可以方便比较的范围。\n我们定义一个把字符串映射到整数的函数 ，这个$f$称为是 Hash 函数。这个函数$f$可以方便地帮我们判断两个字符串是否相等。\n 哈希值不一样的字符串一定不一样 哈希值一样的字符串不一定一样（可能出现哈希碰撞的情况）  字符串哈希 通常采用多项式hash方法，即对于一个长度为$l$的字符串，定义哈希函数为\n$$ f(s)=\\sum_{i=1}^{l} s[i] \\times b^{l-i}(\\bmod M) $$\n例如，字符串$xyz$的哈希值为$xb^2+yb+z$。\nHash的实现 # Python Version M = int(1e9 + 7) B = 233 def get_hash(s): res = 0 for char in s: res = (res * B + ord(char)) % M return res def cmp(s, t): return get_hash(s) == get_hash(t)   错误率分析\n若进行 $n$ 次比较，每次错误率 $\\dfrac 1 M$，那么总错误率是 $1-\\left(1-\\dfrac 1 M\\right)^n$。在随机数据下，若 $M=10^9 + 7$，$n=10^6$，错误率约为 $\\dfrac 1{1000}$，并不是能够完全忽略不计的。\n  改进| 多次询问子串哈希 单次计算一个字符串的哈希时间复杂度为$O(n)$，n为串长。如果多次询问一个字符串的子串的哈希值，每次重新计算效率低下。\n一般采取的方法是对整个字符串先预处理出每个前缀的哈希值，将哈希值看成一个 $b$ 进制的数对 $M$ 取模的结果，这样的话每次就能快速求出子串的哈希了：\n令 $f_i(s)$ 表示 $f(s[1..i])$，即原串长度为 $i$ 的前缀的哈希值，那么按照定义有\n$$ f_i(s)=s[1]\\cdot b^{i-1}+s[2]\\cdot b^{i-2}+\u0026hellip;+s[i-1]\\cdot b+s[i] $$\n现在，我们想要用类似前缀和的方式快速求出 $f(s[l..r])$，按照定义有字符串 $s[l..r]$ 的哈希值为\n$$ f(s[l..r])=s[l]\\cdot b^{r-l}+s[l+1]\\cdot b^{r-l-1}+\u0026hellip;+s[r-1]\\cdot b+s[r] $$\n对比观察上述两个式子，我们发现 $f(s[l..r])=f_r(s)-f_{l-1}(s) \\times b^{r-l+1}$ 成立（可以手动代入验证一下），因此我们用这个式子就可以快速得到子串的哈希值。其中 $b^{r-l+1}$ 可以 $O(n)$ 的预处理出来然后 $O(1)$ 的回答每次询问（当然也可以快速幂 $O(\\log n)$ 的回答每次询问）。\nRabin-Karp 字符串编码  RK字符串编码的思想跟上面多次询问子串哈希一样，以一个仅包含小写英文字母的字符串编码为例。\n 字符串中仅包含小写字母，可以使用arr[i]=ord('i')-ord('a')将字母编码为0-25之间的数字，比如字符串abcde可编码为[0,1,2,3,4]。 将子串看成一个26进制的数，这个数对应的10进制数就是它的编码。  字符串abc的编码为$h_0=0 \\times 26^2 + 1 \\times 26^1 + 2 \\times 26^0 = 28$ 公式表达：对于一个长度为$L$的字符串，设$c_i$为 $s$ 的第 $i$ 个字符编码后的数字，$a$ 为编码的进制，那么有 $h_0 = c_0a^{L-1} + c_1a^{L-2} + \u0026hellip; +c_{L-1}a^0 = \\sum_{i=0}^{L-1} c_ia^{L-1-i}$   在上一步中，对每一个子串的求解编码时间复杂度仍然为$O(n)$，但是当求同一个长度的字符串编码时，可以利用相邻的同长度字符串来求解。   字符串abc的编码为$h_0=0 \\times 26^2 + 1 \\times 26^1 + 2 \\times 26^0 = 28$\n  字符串bcd的编码$h_1=1 \\times 26^2 + 2 \\times 26^1 + 3 \\times 26^0 = 731 $，其实h1可以在h0的基础上，更快的求解出编码，即$h_1=(h_0-0\\times26^2)\\times26+3\\times26^0=731 $\n更一般的表达式为：$h_{1}=\\left(h_{0} \\times a-c_{0} \\times a^{L}\\right)+c_{L+1} $\n  这样即可在常数时间内根据上一个子串求解出下一个子串的字符串编码。\n   当字符个数过多后，字符串编码的数会特别大，一般的做法是需要对编码进行取模来防止溢出，模一般选取编码的信息量的平方的数量级。而取模则会带来哈希碰撞。  为了避免哈希碰撞可以使用两套进制和模的组合。当两套都一样时视作相同。 或者对编码在调用一次内置的hash函数进行哈希。    ","permalink":"http://www.shiluan.space/posts/%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%93%88%E5%B8%8C/","summary":"Hash的思想 Hash 的核心思想在于，将输入映射到一个值域较小、可以方便比较的范围。\n我们定义一个把字符串映射到整数的函数 ，这个$f$称为是 Hash 函数。这个函数$f$可以方便地帮我们判断两个字符串是否相等。\n 哈希值不一样的字符串一定不一样 哈希值一样的字符串不一定一样（可能出现哈希碰撞的情况）  字符串哈希 通常采用多项式hash方法，即对于一个长度为$l$的字符串，定义哈希函数为\n$$ f(s)=\\sum_{i=1}^{l} s[i] \\times b^{l-i}(\\bmod M) $$\n例如，字符串$xyz$的哈希值为$xb^2+yb+z$。\nHash的实现 # Python Version M = int(1e9 + 7) B = 233 def get_hash(s): res = 0 for char in s: res = (res * B + ord(char)) % M return res def cmp(s, t): return get_hash(s) == get_hash(t)   错误率分析\n若进行 $n$ 次比较，每次错误率 $\\dfrac 1 M$，那么总错误率是 $1-\\left(1-\\dfrac 1 M\\right)^n$。在随机数据下，若 $M=10^9 + 7$，$n=10^6$，错误率约为 $\\dfrac 1{1000}$，并不是能够完全忽略不计的。","title":"字符串哈希"},{"content":"控制goroutine的并发数量 chan控制（错误） 在启动go程之前尝试向chan中写入数据.\nfunc main() { userCount := 10 ch := make(chan bool, 2) for i := 0; i \u0026lt; userCount; i++ { // 控制  ch \u0026lt;- true go Read(ch, i) } } func Read(ch chan bool, i int) { fmt.Printf(\u0026#34;go func: %d\\n\u0026#34;, i) \u0026lt;- ch } 这存在一个问题就是可能go程还没执行完,主协程结束子协程也被终止了. 而单纯使用waitgroup没法控制并发数量.\n方案一 : waitgroup+chan组合控制并发   wg主要是防止子协程还没执行完主协程已经退出的情况发生.\n... var wg = sync.WaitGroup{} func main() { userCount := 10 ch := make(chan bool, 2) for i := 0; i \u0026lt; userCount; i++ { wg.Add(1) go Read(ch, i) } wg.Wait() } func Read(ch chan bool, i int) { defer wg.Done() ch \u0026lt;- true fmt.Printf(\u0026#34;go func: %d, time: %d\\n\u0026#34;, i, time.Now().Unix()) time.Sleep(time.Second) \u0026lt;-ch }   这种方案可行,可以将其封装为一个信号量,保护waitgroup和chan的结构体,封装好add方法和Done方法.在每个go程中调用这两个方法.\nvar sema = gsema.NewSemaphore(3) func Read(i int) { defer sema.Done( sema.Add(1) // do something. }   完整的信号量封装示例\npackage gsema import \u0026#34;sync\u0026#34; type Semaphore struct { c chan struct{} wg *sync.WaitGroup } func NewSemaphore(maxSize int) *Semaphore { return \u0026amp;Semaphore{ c: make(chan struct{}, maxSize), wg: new(sync.WaitGroup), } } func (s *Semaphore) Add(delta int) { s.wg.Add(delta) for i := 0; i \u0026lt; delta; i++ { s.c \u0026lt;- struct{}{} } } func (s *Semaphore) Done() { \u0026lt;-s.c s.wg.Done() } func (s *Semaphore) Wait() { s.wg.Wait() }   利\n 适合量不大、复杂度低 的使用场景  几百几千个、几十万个也是可以接受的（看具体业务场景） 实际业务逻辑在运行前就已经被阻塞等待了（因为并发数受限），基本实际业务逻辑损耗的性能比 goroutine 本身大 goroutine 本身很轻便，仅损耗极少许的内存空间和调度。这种等待响应的情况都是躺好了，等待任务唤醒   Semaphore 操作复杂度低且流转简单，容易控制  弊 因为是将所有的goroutine启动后在内部控制其执行.所以还是有弊.\n 不适合量很大、复杂度高 的使用场景  有几百万、几千万个 goroutine 的话，就浪费了大量调度 goroutine 和内存空间。恰好你的服务器也接受不了的话   Semaphore 操作复杂度提高，要管理更多的状态  方案二: 使用waitgroup灵活控制chan 在go程中通过遍历chan来执行任务,在函数外部每次向chan中写入的数量就是允许并发的数量.\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; ) var wg sync.WaitGroup func main() { userCount := 10 ch := make(chan int, 5) for i := 0; i \u0026lt; userCount; i++ { wg.Add(1) go func() { defer wg.Done() for d := range ch { fmt.Printf(\u0026#34;go func: %d, time: %d\\n\u0026#34;, d, time.Now().Unix()) time.Sleep(time.Second * time.Duration(d)) } }() } for i := 0; i \u0026lt; 10; i++ { ch \u0026lt;- 1 ch \u0026lt;- 2 //time.Sleep(time.Second)  } close(ch) wg.Wait() } 方案三:第三方库  Jeffail/tunny panjf2000/ants go-playground/pool nozzle/throttler  比较成熟的第三方库也不少，基本都是以生成和管理 goroutine 为目标的池工具。我简单列了几个，具体建议大家阅读下源码或者多找找，原理相似\nants goroutine池的实现　   golang的net/http标准库中请求处理就用到了goroutine, 并且每来一个请求就开一个goroutine去处理该请求. 源码分析如下.\n  net/http 接收请求且开始处理的源码放在 src/net/http/server.go 里，先从入口函数 ListenAndServe 进去：\nfunc (srv *Server) ListenAndServe() error { addr := srv.Addr if addr == \u0026#34;\u0026#34; { addr = \u0026#34;:http\u0026#34; } ln, err := net.Listen(\u0026#34;tcp\u0026#34;, addr) if err != nil { return err } return srv.Serve(tcpKeepAliveListener{ln.(*net.TCPListener)}) } 看到最后那个 srv.Serve 调用了吗？没错，这个 Serve 方法里面就是实际处理 http 请求的逻辑，我们再进入这个方法内部：\nfunc (srv *Server) Serve(l net.Listener) error { defer l.Close() ... // 不断循环取出TCP连接  for { // 看我看我！！！  rw, e := l.Accept() ... // 再看我再看我！！！  go c.serve(ctx) } } 首先，这个方法的参数(l net.Listener) ，是一个 TCP 监听的封装，负责监听网络端口， rw, e := l.Accept() 则是一个阻塞操作，从网络端口取出一个新的 TCP 连接进行处理，最后 go c.serve(ctx) 就是最后真正去处理这个 http 请求的逻辑了，看到前面的 go 关键字了吗？没错，这里启动了一个新的 goroutine 去执行处理逻辑，而且这是在一个无限循环体里面，所以意味着，每来一个请求它就会开一个 goroutine 去处理，相当任性粗暴啊 …，不过有 Go 调度器背书，一般来说也没啥压力，然而，如果，我是说如果哈，突然一大波请求涌进来了（比方说黑客搞了成千上万的肉鸡 DDOS 你，没错！就这么倒霉！），这时候，就很成问题了，他来 10w 个请求你就要开给他 10w 个 goroutine，来 100w 个你就要老老实实开给他 100w 个，线程调度压力陡升，内存爆满，再然后，你就跪了\n    对于大规模的goroutine调度又两种方案:\n 如同net/http中的方法,每一个请求开一个goroutine. I/O 多路复用 100w 个任务，是不是真的需要 100w 个 goroutine 来处理？未必！用 1w 个 goroutine 也一样可以处理，让一个 goroutine 多处理几个任务就是了嘛，池化的核心优势就在于对 goroutine 的复用。此举首先极大减轻了 runtime 调度 goroutine 的压力，其次，便是降低了对内存的消耗。  实现思路 启动服务之时先初始化一个 Goroutine Pool 池，这个 Pool 维护了一个类似栈的 LIFO 队列 ，里面存放负责处理任务的 Worker，然后在 client 端提交 task 到 Pool 中之后，在 Pool 内部，接收 task 之后的核心操作是：\n 检查当前 Worker 队列中是否有可用的 Worker，如果有，取出执行当前的 task； 没有可用的 Worker，判断当前在运行的 Worker 是否已超过该 Pool 的容量： 是 → 再判断工作池是否为非阻塞模式： 是 → 直接返回 nil， 否 → 阻塞等待直至有 Worker 被放回 Pool 否 →新开一个 Worker（goroutine）处理 每个 Worker 执行完任务之后，放回 Pool 的队列中等待。  此外，该调度系统还有一个清理过期 Worker 的定时任务，该任务在初始化一个 Pool 之时启动，每隔一定的时间间隔去检查空闲 Worker 队列中是否有已经过期的 Worker，有则清理掉，通过定时清理过期 worker，进一步节省系统资源。\n实现细节 https://strikefreedom.top/high-performance-implementation-of-goroutine-pool\n参考 ants作者写的一篇文章:\nhttps://strikefreedom.top/high-performance-implementation-of-goroutine-pool\n","permalink":"http://www.shiluan.space/post/goroutiune%E6%95%B0%E9%87%8F%E6%8E%A7%E5%88%B6/","summary":"控制goroutine的并发数量 chan控制（错误） 在启动go程之前尝试向chan中写入数据.\nfunc main() { userCount := 10 ch := make(chan bool, 2) for i := 0; i \u0026lt; userCount; i++ { // 控制  ch \u0026lt;- true go Read(ch, i) } } func Read(ch chan bool, i int) { fmt.Printf(\u0026#34;go func: %d\\n\u0026#34;, i) \u0026lt;- ch } 这存在一个问题就是可能go程还没执行完,主协程结束子协程也被终止了. 而单纯使用waitgroup没法控制并发数量.\n方案一 : waitgroup+chan组合控制并发   wg主要是防止子协程还没执行完主协程已经退出的情况发生.\n... var wg = sync.WaitGroup{} func main() { userCount := 10 ch := make(chan bool, 2) for i := 0; i \u0026lt; userCount; i++ { wg.","title":"控制goroutine的并发数量"},{"content":"当我们要维护一个有序的序列时，可以使用小顶堆。最近在刷算法题的时候频繁使用，总结笔记如下。\n使用方法    入堆 heapq.heappush(heap, item)\n将 item 的值加入 heap 中，保持堆的不变性。\n  出堆heapq.heappop(heap)\n弹出并返回 heap 的最小的元素，保持堆的不变性。如果堆为空，抛出 IndexError 。使用 heap[0] ，可以只访问最小的元素而不弹出它。\n  有出有入heapq.heappushpop(heap, item)\n将 item 放入堆中，然后弹出并返回 heap 的最小元素。该组合操作比先调用 heappush() 再调用 heappop() 运行起来更有效率。\n  转化为listheapq.heapify(x)\n将list x 转换成堆，原地，线性时间内。\n  替换heapq.heapreplace(heap, item)\n弹出并返回 heap 中最小的一项，同时推入新的 item。 堆的大小不变。 如果堆为空则引发 IndexError。\n  样例   单的单值存储  data=[1,2,3,-3,4,2,4,5] list=[] for v in data: heapq.heappush(list,v) print(\u0026#34;堆:\u0026#34;,list) print(\u0026#34;访问最小值:\u0026#34;,list[0]) print(\u0026#34;访问最大值\u0026#34;,heapq.nlargest(1,list)[0])  小顶堆也可以存储其他元素，如元组、数组。  list=[] heapq.heappush(list,[3,\u0026#34;three\u0026#34;]) heapq.heappush(list,[1,\u0026#34;one\u0026#34;]) heapq.heappush(list,[5,\u0026#34;five\u0026#34;]) print(\u0026#34;堆:\u0026#34;,list) print(\u0026#34;访问最小值:\u0026#34;,list[0]) print(\u0026#34;访问最大值\u0026#34;,heapq.nlargest(1,list)[0]) ### output: #堆: [[1, \u0026#39;one\u0026#39;], [3, \u0026#39;three\u0026#39;], [5, \u0026#39;five\u0026#39;]] #访问最小值: [1, \u0026#39;one\u0026#39;] #访问最大值 [5, \u0026#39;five\u0026#39;] 使用注意事项   若数组X为维护的小顶堆，获取X中的最小值可以使用X[0] 因为内部不是一个有序状态，而是堆数据，访问数组中的最大值不能用X[-1]，可以使用heapq.nlargest(n, iterable, key=None)方法获取前N个最大值。 同理可以使用heapq.nsmallest(n, iterable, key=None) 获取前n个最小值。 不一定要局限于小顶堆，力扣 506题，我们需要根据得分情况对数据从大到小排列。可以将满分减去运动员的分数，在对分数入堆就可以得到一个实际上的“大顶堆”  参考链接   https://docs.python.org/zh-cn/3.8/library/heapq.html  ","permalink":"http://www.shiluan.space/post/python%E5%B0%8F%E9%A1%B6%E5%A0%86%E6%80%BB%E7%BB%93/","summary":"当我们要维护一个有序的序列时，可以使用小顶堆。最近在刷算法题的时候频繁使用，总结笔记如下。\n使用方法    入堆 heapq.heappush(heap, item)\n将 item 的值加入 heap 中，保持堆的不变性。\n  出堆heapq.heappop(heap)\n弹出并返回 heap 的最小的元素，保持堆的不变性。如果堆为空，抛出 IndexError 。使用 heap[0] ，可以只访问最小的元素而不弹出它。\n  有出有入heapq.heappushpop(heap, item)\n将 item 放入堆中，然后弹出并返回 heap 的最小元素。该组合操作比先调用 heappush() 再调用 heappop() 运行起来更有效率。\n  转化为listheapq.heapify(x)\n将list x 转换成堆，原地，线性时间内。\n  替换heapq.heapreplace(heap, item)\n弹出并返回 heap 中最小的一项，同时推入新的 item。 堆的大小不变。 如果堆为空则引发 IndexError。\n  样例   单的单值存储  data=[1,2,3,-3,4,2,4,5] list=[] for v in data: heapq.heappush(list,v) print(\u0026#34;堆:\u0026#34;,list) print(\u0026#34;访问最小值:\u0026#34;,list[0]) print(\u0026#34;访问最大值\u0026#34;,heapq.nlargest(1,list)[0])  小顶堆也可以存储其他元素，如元组、数组。  list=[] heapq.","title":"Python小顶堆总结"},{"content":"Docker常见命令  查看运行容器 docker ps 查看所有容器 docker ps -a 进入ID为 XXXX 的容器 docker exec -it XXXX /bin/bash 停用运行中的全部容器 docker stop $(docker ps -q) 删除全部容器 docker rm $(docker ps -aq) 停用并删除全部容器 docker stop $(docker ps -q) \u0026amp; docker rm $(docker ps -qa) 使用Dockerfile构建镜像：docker build -t name . 查看所有镜像 docker images docker 删除现存的镜像 docker rmi XXX  Docker应用部署 部署golang项目 Dockerfile参考\nFROM golang:latest WORKDIR $GOPATH/src/github.com/EDDYCJY/go-gin-example COPY . $GOPATH/src/github.com/EDDYCJY/go-gin-example RUN go build . EXPOSE 8000 ENTRYPOINT [\u0026#34;./go-gin-example\u0026#34;] 其他 相关教程网站 更换docker镜像源   打开配置文件\nlinux路径：/etc/docker/daemon.json\nwindows路径：C:\\Users\\shilu\\.docker\\daemon.json\n  添加配置文件\n  { \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://almtd3fa.mirror.aliyuncs.com\u0026#34;] }  重启docker服务  docker 构建go应用速度慢  解决git安装慢的问题：给 Git 指定代理，在 Dockerfile 中添加以下内容  RUN git config --global https.proxy http://127.0.0.1:1080 RUN git config --global https.proxy https://127.0.0.1:1080   解决 golang.org 依赖下载失败的问题\n修改Dockerfile中的编译命令\nRUN GOPROXY=\u0026quot;https://goproxy.io\u0026quot; go build -o main endgame/main.go\n  Docker push上传镜像失败 Docker hub被墙，不稳定，所以容易出现超时，解决方案如下\n 使用国内阿里云docker或者DaoCloud(目前创建docker仓库是收费的) 多次重试  Docker Login账号密码正确仍然登陆不上  在后台配置一个访问密钥 使用密钥登陆即可。  Docker Login登陆报证书错误 docker login 出现错误，提示：Error saving credentials: error storing credentials - err: exit status 1, out: Cannot autolaunch D-Bus without X11 $DISPLAY\n解决办法：\n 首先安装 gnupg2 和 pass 包，并生成 gpg2 key （我没有用到生成步骤一样可行）  sudo apt install gnupg2 pass gpg2 --full-generate-key  查看生成的 key ，使用 pass 加载验证  gpg2 -k pass init \u0026quot;whatever key id you have\u0026quot; 这里引号中 要填写前面给出的密钥\n做完上述操作后，再使用 docker login 就没有问题了。\nLinux中的注意事项 Docker 需要用户具有 sudo 权限，为了避免每次命令都输入sudo，可以把用户加入 Docker 用户组。sudo usermod -aG docker $USER\n","permalink":"http://www.shiluan.space/post/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","summary":"Docker常见命令  查看运行容器 docker ps 查看所有容器 docker ps -a 进入ID为 XXXX 的容器 docker exec -it XXXX /bin/bash 停用运行中的全部容器 docker stop $(docker ps -q) 删除全部容器 docker rm $(docker ps -aq) 停用并删除全部容器 docker stop $(docker ps -q) \u0026amp; docker rm $(docker ps -qa) 使用Dockerfile构建镜像：docker build -t name . 查看所有镜像 docker images docker 删除现存的镜像 docker rmi XXX  Docker应用部署 部署golang项目 Dockerfile参考\nFROM golang:latest WORKDIR $GOPATH/src/github.com/EDDYCJY/go-gin-example COPY . $GOPATH/src/github.com/EDDYCJY/go-gin-example RUN go build . EXPOSE 8000 ENTRYPOINT [\u0026#34;.","title":"Docker学习笔记"},{"content":"前言 Chocolatey是windows下的软件包管理工具，使用chocolatey可以快速的安装软件。\n安装 使用管理员权限模式打开powershell，粘贴下方安装代码，稍等片刻即可安装成功。\nSet-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString(\u0026#39;https://chocolatey.org/install.ps1\u0026#39;)) 安装完成后输入choco -v 如何出现版本号即表示安装成功。\n使用 choco 常见命令 choco search \u0026lt;keyword\u0026gt; 搜索软件 choco list \u0026lt;keyword\u0026gt; 跟 search 命令功能类似 choco install \u0026lt;package1 package2 package3...\u0026gt; 安装软件 choco install \u0026lt;package\u0026gt; -version *** 安装指定版本 choco uninstall name 卸载软件 choco version \u0026lt;package\u0026gt; 查看安装包的版本情况 choco upgrade \u0026lt;package\u0026gt; 更新某个软件 choco list -localonly 查看一下所有安装在本地的包的列表 choco list -lo 功能同上 choco安装软件 choco官网可以查询可安装的软件包：https://community.chocolatey.org/packages\n常见软件：\n 7-zip : choco install 7zip.install git ：choco install git  go : choco install golang vscode: choco install vscode vim : choco install vim docker : choco install docker-cli docker-machine docker-compose docker-desktop docker-toolboxa  ","permalink":"http://www.shiluan.space/post/20211208-2/","summary":"前言 Chocolatey是windows下的软件包管理工具，使用chocolatey可以快速的安装软件。\n安装 使用管理员权限模式打开powershell，粘贴下方安装代码，稍等片刻即可安装成功。\nSet-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString(\u0026#39;https://chocolatey.org/install.ps1\u0026#39;)) 安装完成后输入choco -v 如何出现版本号即表示安装成功。\n使用 choco 常见命令 choco search \u0026lt;keyword\u0026gt; 搜索软件 choco list \u0026lt;keyword\u0026gt; 跟 search 命令功能类似 choco install \u0026lt;package1 package2 package3...\u0026gt; 安装软件 choco install \u0026lt;package\u0026gt; -version *** 安装指定版本 choco uninstall name 卸载软件 choco version \u0026lt;package\u0026gt; 查看安装包的版本情况 choco upgrade \u0026lt;package\u0026gt; 更新某个软件 choco list -localonly 查看一下所有安装在本地的包的列表 choco list -lo 功能同上 choco安装软件 choco官网可以查询可安装的软件包：https://community.chocolatey.org/packages\n常见软件：\n 7-zip : choco install 7zip.","title":"windows 安装Chocolatey"},{"content":"前言 之前在做一个计算机实训的时候，装docker环境报各种错误，今天尝试使用choco包管理工具，2行命令安装成功！\nchoco安装 Chocolatey是windows下的软件包管理工具，使用chocolatey可以快速的安装软件。\n使用管理员权限模式打开powershell，粘贴下方安装代码，稍等片刻即可安装成功。\nSet-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString(\u0026#39;https://chocolatey.org/install.ps1\u0026#39;)) 安装完成后输入choco -v 如何出现版本号即表示安装成功。\ndocker 安装 在管理员cmd下输入如下命令\nchoco install docker-cli docker-machine docker-compose docker-desktop docker-toolbox 输入后会提示是否执行脚本，输入a回车，即可。\n测试安装情况 安装上述五个软件包后，需要自行启动docker-desktop软件，如果没有报错则安装成功。\n或者新开一个cmd窗口，输入命令docker-info, 如果没有保存则表示安装成功！\n常见报错  如果报如下图所示提示WSL 2 installation is incomplete ，是因为电脑没有安装linux子系统，   - 同样建议使用choco安装，输入命令choco install wsl2 安装后重试。\n","permalink":"http://www.shiluan.space/post/20211208-1/","summary":"前言 之前在做一个计算机实训的时候，装docker环境报各种错误，今天尝试使用choco包管理工具，2行命令安装成功！\nchoco安装 Chocolatey是windows下的软件包管理工具，使用chocolatey可以快速的安装软件。\n使用管理员权限模式打开powershell，粘贴下方安装代码，稍等片刻即可安装成功。\nSet-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString(\u0026#39;https://chocolatey.org/install.ps1\u0026#39;)) 安装完成后输入choco -v 如何出现版本号即表示安装成功。\ndocker 安装 在管理员cmd下输入如下命令\nchoco install docker-cli docker-machine docker-compose docker-desktop docker-toolbox 输入后会提示是否执行脚本，输入a回车，即可。\n测试安装情况 安装上述五个软件包后，需要自行启动docker-desktop软件，如果没有报错则安装成功。\n或者新开一个cmd窗口，输入命令docker-info, 如果没有保存则表示安装成功！\n常见报错  如果报如下图所示提示WSL 2 installation is incomplete ，是因为电脑没有安装linux子系统，   - 同样建议使用choco安装，输入命令choco install wsl2 安装后重试。","title":"windows下使用choco极速安装docker"},{"content":"Go 切片数据结构 Go 中常用的切片（Slice）数据结构是一枚动态数组，提供方便的局部索引功能，切片长度并不固定，并且会在容量不足时自动扩容。\n切片实质上是对一个底层数组的抽象视图 ，由 Go 运行时维护。在运行时，切片由如下的SliceHeader结构体表示，其中Data字段是指向底层数组的指针，Len表示当前切片的长度，而Cap表示当前切片的容量，也就是Data数组的大小。\ntype SliceHeader struct { Data uintptr Len int Cap int }  代码示例：Go 切片 Slice 数据结构\n 切片作为函数入参 许多 Go 开发者对传递切片作为函数入参的习惯性认知是：**传递切片，等同于传递指针，函数内部对切片的修改，将会影响到函数外部的切片。**这一习惯性认知在大部分情况下都是正确的。如以下代码所示：在modify函数中修改切片，外部main函数中的切片受到了影响。\npackage main import ( \u0026quot;fmt\u0026quot; ) func modify(s []string) { for i := 0; i \u0026lt; len(s); i++ { s[i] = \u0026quot;b\u0026quot; } fmt.Println(\u0026quot;Inner:\u0026quot;, s) } func main() { s := []string{\u0026quot;a1\u0026quot;, \u0026quot;a2\u0026quot;} fmt.Println(\u0026quot;Before:\u0026quot;, s) modify(s) fmt.Println(\u0026quot;After:\u0026quot;, s) }  代码示例：Go 切片作为函数入参\n Before: [a1 a2] Inner: [b b] After: [b b]  代码示例：Go 切片作为函数入参 - 程序输出\n 我们对示例代码做一些修改，在内部函数中触发切片的扩容机制 ，事情看起来就非常有趣了：函数内部对切片的修改并没有影响到函数外部的切片。\nfunc modify(s []string) { + s = append(s, \u0026quot;b\u0026quot;) for i := 0; i \u0026lt; len(s); i++ { s[i] = \u0026quot;b\u0026quot; } fmt.Println(\u0026quot;Inner:\u0026quot;, s) }  代码示例：Go 切片作为函数入参（切片扩容）\n Before: [a1 a2] Inner: [b b b] After: [a1 a2]  代码示例：Go 切片作为函数入参（切片扩容）- 程序输出\n 解释 在 Go 中，函数参数传递机制为值拷贝 。\n将切片作为函数参数传递，实际上是拷贝了SliceHeader结构体传入函数，结构体包含一枚指向底层数组的指针，因此在函数内修改切片，操作的底层数组是相同的。\n但是如果函数内的切片操作触发了切片扩容（如：使用append追加元素），Go 运行时会为切片分配一块新的内存空间并将原切片的所有元素拷贝过去，函数内部切片的底层数组指针指向了新分配 的内存空间，而函数外部切片底层数组指针仍指向分配前 的地址空间，由此出现了内外切片不一致的有趣情形。\n我们可以通过一个简单的方式验证扩容前后切片的变化：打印扩容前后切片数组的内存地址。在函数外部打印切片的数组地址，会发现其与切片扩容前地址相同。\nfunc modify(s []string) { + fmt.Printf(\u0026quot;Before grow slice, \u0026amp;s[0]: %p\\n\u0026quot;, \u0026amp;s[0]) s = append(s, \u0026quot;b\u0026quot;) for i := 0; i \u0026lt; len(s); i++ { s[i] = \u0026quot;b\u0026quot; } fmt.Println(\u0026quot;Inner:\u0026quot;, s) + fmt.Printf(\u0026quot;After grow slice, \u0026amp;s[0]: %p\\n\u0026quot;, \u0026amp;s[0]) }  代码示例：打印扩容前后切片数组地址（首元素地址）\n 建议 理解了 Go 中切片作为函数参数传递的内部原理后，如何在代码中正确运用切片传参也就比较明晰了。\n  操作不涉及切片容量变化，直接传递切片。\n  操作涉及切片容量变化，且需要反馈给调用方，传递切片指针。\n  参考资料   Arrays, slices (and strings): The mechanics of ‘append’：https://blog.golang.org/slices\n  Go 语言设计与实现：https://draveness.me/golang/docs/part2-foundation/ch03-datastructure/golang-array-and-slice/\n  ","permalink":"http://www.shiluan.space/post/go%E5%88%87%E7%89%87%E4%BC%A0%E5%85%A5%E9%A3%8E%E9%99%A9/","summary":"Go 切片数据结构 Go 中常用的切片（Slice）数据结构是一枚动态数组，提供方便的局部索引功能，切片长度并不固定，并且会在容量不足时自动扩容。\n切片实质上是对一个底层数组的抽象视图 ，由 Go 运行时维护。在运行时，切片由如下的SliceHeader结构体表示，其中Data字段是指向底层数组的指针，Len表示当前切片的长度，而Cap表示当前切片的容量，也就是Data数组的大小。\ntype SliceHeader struct { Data uintptr Len int Cap int }  代码示例：Go 切片 Slice 数据结构\n 切片作为函数入参 许多 Go 开发者对传递切片作为函数入参的习惯性认知是：**传递切片，等同于传递指针，函数内部对切片的修改，将会影响到函数外部的切片。**这一习惯性认知在大部分情况下都是正确的。如以下代码所示：在modify函数中修改切片，外部main函数中的切片受到了影响。\npackage main import ( \u0026quot;fmt\u0026quot; ) func modify(s []string) { for i := 0; i \u0026lt; len(s); i++ { s[i] = \u0026quot;b\u0026quot; } fmt.Println(\u0026quot;Inner:\u0026quot;, s) } func main() { s := []string{\u0026quot;a1\u0026quot;, \u0026quot;a2\u0026quot;} fmt.Println(\u0026quot;Before:\u0026quot;, s) modify(s) fmt.Println(\u0026quot;After:\u0026quot;, s) }  代码示例：Go 切片作为函数入参","title":"Go 传递切片入参的隐藏风险"},{"content":"作为一个Web应用开发者，在选择密码存储方案时, 容易掉入哪些陷阱, 以及如何避免这些陷阱?\n普通方案 目前用的最多的密码存储方案是将明文密码做单向哈希后存储，单向哈希算法有一个特征：无法通过哈希后的摘要(digest)恢复原始数据，这也是“单向”二字的来源。常用的单向哈希算法包括SHA-256, SHA-1, MD5等。\nGo语言对这三种加密算法的实现如下所示：\n//import \u0026#34;crypto/sha256\u0026#34; h := sha256.New() io.WriteString(h, \u0026#34;His money is twice tainted: \u0026#39;taint yours and \u0026#39;taint mine.\u0026#34;) fmt.Printf(\u0026#34;% x\u0026#34;, h.Sum(nil)) //import \u0026#34;crypto/sha1\u0026#34; h := sha1.New() io.WriteString(h, \u0026#34;His money is twice tainted: \u0026#39;taint yours and \u0026#39;taint mine.\u0026#34;) fmt.Printf(\u0026#34;% x\u0026#34;, h.Sum(nil)) //import \u0026#34;crypto/md5\u0026#34; h := md5.New() io.WriteString(h, \u0026#34;需要加密的密码\u0026#34;) fmt.Printf(\u0026#34;%x\u0026#34;, h.Sum(nil)) 单向哈希有两个特性：\n 1）同一个密码进行单向哈希，得到的总是唯一确定的摘要。 2）计算速度快。随着技术进步，一秒钟能够完成数十亿次单向哈希计算。  结合上面两个特点，考虑到多数人所使用的密码为常见的组合，攻击者可以将所有密码的常见组合进行单向哈希，得到一个摘要组合, 然后与数据库中的摘要进行比对即可获得对应的密码。这个摘要组合也被称为rainbow table。\n因此通过单向加密之后存储的数据，和明文存储没有多大区别。因此，一旦网站的数据库泄露，所有用户的密码本身就大白于天下。\n进阶方案 通过上面介绍我们知道黑客可以用rainbow table来破解哈希后的密码，很大程度上是因为加密时使用的哈希算法是公开的。如果黑客不知道加密的哈希算法是什么，那他也就无从下手了。\n一个直接的解决办法是，自己设计一个哈希算法。然而，一个好的哈希算法是很难设计的——既要避免碰撞，又不能有明显的规律，做到这两点要比想象中的要困难很多。因此实际应用中更多的是利用已有的哈希算法进行多次哈希。\n但是单纯的多次哈希，依然阻挡不住黑客。两次 MD5、三次 MD5之类的方法，我们能想到，黑客自然也能想到。特别是对于一些开源代码，这样哈希更是相当于直接把算法告诉了黑客。\n现在安全性比较好的网站，都会用一种叫做“加盐”的方式来存储密码，也就是常说的 “salt”。他们通常的做法是，先将用户输入的密码进行一次MD5（或其它哈希算法）加密；将得到的 MD5 值前后加上一些只有管理员自己知道的随机串，再进行一次MD5加密。这个随机串中可以包括某些固定的串，也可以包括用户名（用来保证每个用户加密使用的密钥都不一样）。\n//import \u0026#34;crypto/md5\u0026#34; //假设用户名abc，密码123456 h := md5.New() io.WriteString(h, \u0026#34;需要加密的密码\u0026#34;) //pwmd5等于e10adc3949ba59abbe56e057f20f883e pwmd5 :=fmt.Sprintf(\u0026#34;%x\u0026#34;, h.Sum(nil)) //指定两个 salt： salt1 = @#$% salt2 = ^\u0026amp;*() salt1 := \u0026#34;@#$%\u0026#34; salt2 := \u0026#34;^\u0026amp;*()\u0026#34; //salt1+用户名+salt2+MD5拼接 io.WriteString(h, salt1) io.WriteString(h, \u0026#34;abc\u0026#34;) io.WriteString(h, salt2) io.WriteString(h, pwmd5) last :=fmt.Sprintf(\u0026#34;%x\u0026#34;, h.Sum(nil)) 在两个salt没有泄露的情况下，黑客如果拿到的是最后这个加密串，就几乎不可能推算出原始的密码是什么了。\n专家方案 上面的进阶方案在几年前也许是足够安全的方案，因为攻击者没有足够的资源建立这么多的rainbow table。 但是，时至今日，因为并行计算能力的提升，这种攻击已经完全可行。\n怎么解决这个问题呢？只要时间与资源允许，没有破译不了的密码，所以方案是:故意增加密码计算所需耗费的资源和时间，使得任何人都不可获得足够的资源建立所需的rainbow table。\n这类方案有一个特点，算法中都有个因子，用于指明计算密码摘要所需要的资源和时间，也就是计算强度。计算强度越大，攻击者建立rainbow table越困难，以至于不可继续。\n这里推荐scrypt方案，scrypt是由著名的FreeBSD黑客Colin Percival为他的备份服务Tarsnap开发的。\n目前Go语言里面支持的库 https://github.com/golang/crypto/tree/master/scrypt\ndk := scrypt.Key([]byte(\u0026#34;some password\u0026#34;), []byte(salt), 16384, 8, 1, 32) 通过上面的方法可以获取唯一的相应的密码值，这是目前为止最难破解的。\n总结  使用专家方案这里推荐scrypt方案，scrypt是由著名的FreeBSD黑客Colin Percival为他的备份服务Tarsnap开发的。https://pkg.go.dev/golang.org/x/crypto/scrypt 使用另外一种比较成熟的方案 https://pkg.go.dev/golang.org/x/crypto/bcrypt  ","permalink":"http://www.shiluan.space/post/%E5%90%8E%E7%AB%AF%E5%AF%86%E7%A0%81%E5%AD%98%E5%82%A8%E6%96%B9%E6%A1%88/","summary":"作为一个Web应用开发者，在选择密码存储方案时, 容易掉入哪些陷阱, 以及如何避免这些陷阱?\n普通方案 目前用的最多的密码存储方案是将明文密码做单向哈希后存储，单向哈希算法有一个特征：无法通过哈希后的摘要(digest)恢复原始数据，这也是“单向”二字的来源。常用的单向哈希算法包括SHA-256, SHA-1, MD5等。\nGo语言对这三种加密算法的实现如下所示：\n//import \u0026#34;crypto/sha256\u0026#34; h := sha256.New() io.WriteString(h, \u0026#34;His money is twice tainted: \u0026#39;taint yours and \u0026#39;taint mine.\u0026#34;) fmt.Printf(\u0026#34;% x\u0026#34;, h.Sum(nil)) //import \u0026#34;crypto/sha1\u0026#34; h := sha1.New() io.WriteString(h, \u0026#34;His money is twice tainted: \u0026#39;taint yours and \u0026#39;taint mine.\u0026#34;) fmt.Printf(\u0026#34;% x\u0026#34;, h.Sum(nil)) //import \u0026#34;crypto/md5\u0026#34; h := md5.New() io.WriteString(h, \u0026#34;需要加密的密码\u0026#34;) fmt.Printf(\u0026#34;%x\u0026#34;, h.Sum(nil)) 单向哈希有两个特性：\n 1）同一个密码进行单向哈希，得到的总是唯一确定的摘要。 2）计算速度快。随着技术进步，一秒钟能够完成数十亿次单向哈希计算。  结合上面两个特点，考虑到多数人所使用的密码为常见的组合，攻击者可以将所有密码的常见组合进行单向哈希，得到一个摘要组合, 然后与数据库中的摘要进行比对即可获得对应的密码。这个摘要组合也被称为rainbow table。\n因此通过单向加密之后存储的数据，和明文存储没有多大区别。因此，一旦网站的数据库泄露，所有用户的密码本身就大白于天下。\n进阶方案 通过上面介绍我们知道黑客可以用rainbow table来破解哈希后的密码，很大程度上是因为加密时使用的哈希算法是公开的。如果黑客不知道加密的哈希算法是什么，那他也就无从下手了。\n一个直接的解决办法是，自己设计一个哈希算法。然而，一个好的哈希算法是很难设计的——既要避免碰撞，又不能有明显的规律，做到这两点要比想象中的要困难很多。因此实际应用中更多的是利用已有的哈希算法进行多次哈希。\n但是单纯的多次哈希，依然阻挡不住黑客。两次 MD5、三次 MD5之类的方法，我们能想到，黑客自然也能想到。特别是对于一些开源代码，这样哈希更是相当于直接把算法告诉了黑客。","title":"后端密码存储方案"},{"content":"因为实验室的PC和宿舍的笔记本数据无法互通，所以将Syncthing部署在服务器上备份同步代码文件等重要的小文件。\n下载部署在Linux服务器上  从官网下载Linux平台部署包   v1.18.2 备份\n syncthing-linux-amd64-v1.18.2.tar.gz\n 将文件复制到/usr/bin目录下   sudo cp syncthing /usr/bin/syncthing\n 先把服务跑一遍，让他自动生成配置文件   ./usr/bin/syncthing\n 服务默认是只能局域网访问，需要更改配置文件， 配置文件路径为/home/你的用户/.config/syncthing/config.xml   找到配置文件将address部分改成0.0.0.0/8384\n 放通8384端口 即可直接访问服务器的syncthing后台页面   记得设置密码！\n配置安装 windows平台安装 https://github.com/canton7/SyncTrayzor/releases\n在github下载图形界面安装包安装打开\n配置 默认只有127.0.0.1，localhost可以访问，没有远程访问权限。\n照下图，右边的设置，用来修改图形界面监听地址，并且可以设置随开机自动启动\n连接到服务器 添加后在云盘的管理界面接受即可\n添加需要同步的文件夹 点击添加文件夹 ，配置文件夹路径，根据自己的需求配置忽略模式 。 最后在共享里勾选共享到你的云盘\n在云盘中接受同步请求\n两个笔记本之间的数据同步 在另外一个笔记本中安装客户端，在服务器后台将对应的文件夹共享给你的设备即可。\n其实不需要服务器中转也可以，但是这要求你的两台电脑都在线而且速率会有限制。 利用服务器，备份+同步都达到了。\n","permalink":"http://www.shiluan.space/post/syncthing/","summary":"因为实验室的PC和宿舍的笔记本数据无法互通，所以将Syncthing部署在服务器上备份同步代码文件等重要的小文件。\n下载部署在Linux服务器上  从官网下载Linux平台部署包   v1.18.2 备份\n syncthing-linux-amd64-v1.18.2.tar.gz\n 将文件复制到/usr/bin目录下   sudo cp syncthing /usr/bin/syncthing\n 先把服务跑一遍，让他自动生成配置文件   ./usr/bin/syncthing\n 服务默认是只能局域网访问，需要更改配置文件， 配置文件路径为/home/你的用户/.config/syncthing/config.xml   找到配置文件将address部分改成0.0.0.0/8384\n 放通8384端口 即可直接访问服务器的syncthing后台页面   记得设置密码！\n配置安装 windows平台安装 https://github.com/canton7/SyncTrayzor/releases\n在github下载图形界面安装包安装打开\n配置 默认只有127.0.0.1，localhost可以访问，没有远程访问权限。\n照下图，右边的设置，用来修改图形界面监听地址，并且可以设置随开机自动启动\n连接到服务器 添加后在云盘的管理界面接受即可\n添加需要同步的文件夹 点击添加文件夹 ，配置文件夹路径，根据自己的需求配置忽略模式 。 最后在共享里勾选共享到你的云盘\n在云盘中接受同步请求\n两个笔记本之间的数据同步 在另外一个笔记本中安装客户端，在服务器后台将对应的文件夹共享给你的设备即可。\n其实不需要服务器中转也可以，但是这要求你的两台电脑都在线而且速率会有限制。 利用服务器，备份+同步都达到了。","title":"Syncthing 数据网盘部署"},{"content":"起因：实验室的打印机抽风了，只有我的电脑连的上其他人的电脑都连不上。可能是我离打印机最近。\npython的win32print包可以将打印任务直接发送到打印机，于是写了个web服务，接受来自本地小伙伴的文件，然后将其打印。\nwin32print 通过win32print.EnumPrinters(2)读取到你电脑上连接的所有打印机，然后设置成默认打印机，建议先执行一次，选择固定的。\ndef PrintFile(file_path): allPrinter=[printer[2] for printer in win32print.EnumPrinters(2)] # PrintNum = int(input(\u0026#34;选择打印机:\\n\u0026#34;+\u0026#34;\\n\u0026#34;.join([f\u0026#34;{n} {p}\u0026#34; for n,p in enumerate(allPrinter)]))) win32print.SetDefaultPrinter(allPrinter[4]) # pdf_path=\u0026#34;C:\\\\Users\\Luke\\Desktop\\Doc1.docx\u0026#34; pdf_path=file_path win32api.ShellExecute(0, \u0026#34;print\u0026#34;, pdf_path, \u0026#39;/d:\u0026#34;%s\u0026#34;\u0026#39; % win32print.GetDefaultPrinter (), \u0026#34;.\u0026#34;, 0) 如果出现pywintypes.error: (31, \u0026lsquo;ShellExecute\u0026rsquo;, \u0026lsquo;连到系统上的设备没有发挥作用。') 错误，需要为你的pdf或者其他文件指定一个默认打开应用。\nWeb服务 使用flask写了个小demo, 接受局域网的文件后调用打印机打印。\nslash = \u0026#39;\\\\\u0026#39; UPLOAD_FOLDER = \u0026#39;upload\u0026#39; ALLOW_EXTENSIONS = {\u0026#39;doc\u0026#39;, \u0026#39;docx\u0026#39;, \u0026#39;pdf\u0026#39;} app = Flask(__name__) app.config[\u0026#39;UPLOAD_FOLDER\u0026#39;] = UPLOAD_FOLDER #判断文件夹是否存在，如果不存在则创建 if not os.path.exists(UPLOAD_FOLDER): os.makedirs(UPLOAD_FOLDER) else: pass # 判断文件后缀是否在列表中 def allowed_file(filename): return \u0026#39;.\u0026#39; in filename and \\ filename.rsplit(\u0026#39;.\u0026#39;, 1)[1] in ALLOW_EXTENSIONS @app.route(\u0026#39;/\u0026#39;,methods=[\u0026#39;GET\u0026#39;,\u0026#39;POST\u0026#39;]) def upload_file(): if request.method ==\u0026#39;POST\u0026#39;: #获取post过来的文件名称，从name=file参数中获取 file = request.files[\u0026#39;file\u0026#39;] if file and allowed_file(file.filename): # secure_filename方法会去掉文件名中的中文 filename = secure_filename(file.filename) #因为上次的文件可能有重名，因此使用uuid保存文件 file_name = str(uuid.uuid4()) + \u0026#39;.\u0026#39; + filename.rsplit(\u0026#39;.\u0026#39;, 1)[1] file.save(os.path.join(app.config[\u0026#39;UPLOAD_FOLDER\u0026#39;],file_name)) base_path = os.getcwd() file_path = base_path + slash + app.config[\u0026#39;UPLOAD_FOLDER\u0026#39;] + slash + file_name PrintFile(file_path) return redirect(url_for(\u0026#39;upload_file\u0026#39;,filename = file_name)) return render_template(\u0026#39;index.html\u0026#39;)  其中使用的index模板文件如下  \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;310Lab Printer\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;h1\u0026gt;上传文件并打印\u0026lt;/h1\u0026gt; \u0026lt;form action=\u0026#34;\u0026#34; method=\u0026#34;post\u0026#34; enctype=multipart/form-data\u0026gt; \u0026lt;p\u0026gt;\u0026lt;input type=file name=file\u0026gt; \u0026lt;input type=submit value=Upload\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;!--\u0026lt;p\u0026gt;仅支持doc,docx,pdf文件\u0026lt;/p\u0026gt;--\u0026gt; \u0026lt;body\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 源码  https://github.com/shiluanzzz/webprint  参考   https://stackoverflow.com/questions/39249360/python-print-pdf-file-with-win32print\n  https://stackoverflow.com/questions/39249360/python-print-pdf-file-with-win32print\n  ","permalink":"http://www.shiluan.space/post/%E5%B1%80%E5%9F%9F%E7%BD%91%E6%89%93%E5%8D%B0%E6%9C%8D%E5%8A%A1/","summary":"起因：实验室的打印机抽风了，只有我的电脑连的上其他人的电脑都连不上。可能是我离打印机最近。\npython的win32print包可以将打印任务直接发送到打印机，于是写了个web服务，接受来自本地小伙伴的文件，然后将其打印。\nwin32print 通过win32print.EnumPrinters(2)读取到你电脑上连接的所有打印机，然后设置成默认打印机，建议先执行一次，选择固定的。\ndef PrintFile(file_path): allPrinter=[printer[2] for printer in win32print.EnumPrinters(2)] # PrintNum = int(input(\u0026#34;选择打印机:\\n\u0026#34;+\u0026#34;\\n\u0026#34;.join([f\u0026#34;{n} {p}\u0026#34; for n,p in enumerate(allPrinter)]))) win32print.SetDefaultPrinter(allPrinter[4]) # pdf_path=\u0026#34;C:\\\\Users\\Luke\\Desktop\\Doc1.docx\u0026#34; pdf_path=file_path win32api.ShellExecute(0, \u0026#34;print\u0026#34;, pdf_path, \u0026#39;/d:\u0026#34;%s\u0026#34;\u0026#39; % win32print.GetDefaultPrinter (), \u0026#34;.\u0026#34;, 0) 如果出现pywintypes.error: (31, \u0026lsquo;ShellExecute\u0026rsquo;, \u0026lsquo;连到系统上的设备没有发挥作用。') 错误，需要为你的pdf或者其他文件指定一个默认打开应用。\nWeb服务 使用flask写了个小demo, 接受局域网的文件后调用打印机打印。\nslash = \u0026#39;\\\\\u0026#39; UPLOAD_FOLDER = \u0026#39;upload\u0026#39; ALLOW_EXTENSIONS = {\u0026#39;doc\u0026#39;, \u0026#39;docx\u0026#39;, \u0026#39;pdf\u0026#39;} app = Flask(__name__) app.config[\u0026#39;UPLOAD_FOLDER\u0026#39;] = UPLOAD_FOLDER #判断文件夹是否存在，如果不存在则创建 if not os.path.exists(UPLOAD_FOLDER): os.makedirs(UPLOAD_FOLDER) else: pass # 判断文件后缀是否在列表中 def allowed_file(filename): return \u0026#39;.","title":"局域网打印服务"},{"content":"安装 pip install openpyxl\n**英文文档链接 ： **点击这里\n 1，定位excel 新建excel和打开现有的excel\nimport openpyxl wb1 = openpyxl.load_workbook(\u0026#39;text.xlsx\u0026#39;) #定位到现有的excle  wb2= openpyxl.Workbook() #创建一个新的excle表 （其中没有任何工作簿！！！） 定位到具体的工作表（因为一个excle中可能存在多个工作表）\nbo1=wb1.active #定位到 wb1 指向的excle 的第一个工作簿  bo2=wb1.get_sheet_by_name(\u0026#34;name\u0026#34;) #定位到 名为 name 的工作簿  bo3=wb1[\u0026#34;new title\u0026#34;] #定位到名为 new title 的工作簿 2，读取excle中的内容 使用openpyxl.wordsheet.Worksheet.cell方法从单元格中读取内容\n#读取内容  #利用openpyxl读取内容主要是从每个单元格中 读取  #定位到单元格的方法  #使用value属性获取当前单元格的值  content=bo1[\u0026#39;A4\u0026#39;].value content3=bo1.cell(row=4,column=2).value #也可以使用迭代 多个单元格在使用value方法，类似于list的切片  content4=bo1[\u0026#39;A1\u0026#39;:\u0026#39;C2\u0026#39;] #使用  3，写入,修改内容 3.1 针对单个单元格写入 参考代码\nimport openpyxl work=openpyxl.load_workbook(\u0026#34;dat.xlsx\u0026#34;) book=work.active print(\u0026#34;before:\u0026#34;,book.cell(1,1).value) book[\u0026#39;A1\u0026#39;].value=\u0026#34;1\u0026#34; #book.cell(1,1).value=\u0026#34;1\u0026#34; 效果等同于上调语句  print(\u0026#34;after:\u0026#34;,book.cell(1,1).value) work.save(\u0026#34;123.xlsx\u0026#34;)#一定记得保存！！！ 3.2 针对多个单元格写入 参考代码\nimport openpyxl work=openpyxl.load_workbook(\u0026#34;123.xlsx\u0026#34;) book=work.active # print(\u0026#34;before:\u0026#34;,book.cell(1,1).value)  # book[\u0026#39;A1\u0026#39;].value=\u0026#34;1\u0026#34;  # print(\u0026#34;after:\u0026#34;,book.cell(1,1).value)  # work.save(\u0026#34;123.xlsx\u0026#34;)  for a in range(1,6): #写入第1行至第5行  for b in range(1,11): #迭代第1列到第10列  #下面三条语句的效果都是一样的  book.cell(a,b).value=str(a)+\u0026#34;:\u0026#34;+str(b) # book.cell(row=a,column=b).value=str(a)+\u0026#34;:\u0026#34;+str(b)  # book.cell(row=a, column=b,value=\u0026#34;{}\u0026#34;.format(str(a)+\u0026#34;:\u0026#34;+str(b)))  work.save(\u0026#34;1.xlsx\u0026#34;) 4，文件保存 通过 openpyxl.workbook,Workbook.save()方法保存文件是最简单最安全的：\nwb.save('balances.xlsx') 5, 其他  excel多少行sheet.max_row  ","permalink":"http://www.shiluan.space/post/python%E5%A4%84%E7%90%86excel%E6%96%87%E4%BB%B6/","summary":"安装 pip install openpyxl\n**英文文档链接 ： **点击这里\n 1，定位excel 新建excel和打开现有的excel\nimport openpyxl wb1 = openpyxl.load_workbook(\u0026#39;text.xlsx\u0026#39;) #定位到现有的excle  wb2= openpyxl.Workbook() #创建一个新的excle表 （其中没有任何工作簿！！！） 定位到具体的工作表（因为一个excle中可能存在多个工作表）\nbo1=wb1.active #定位到 wb1 指向的excle 的第一个工作簿  bo2=wb1.get_sheet_by_name(\u0026#34;name\u0026#34;) #定位到 名为 name 的工作簿  bo3=wb1[\u0026#34;new title\u0026#34;] #定位到名为 new title 的工作簿 2，读取excle中的内容 使用openpyxl.wordsheet.Worksheet.cell方法从单元格中读取内容\n#读取内容  #利用openpyxl读取内容主要是从每个单元格中 读取  #定位到单元格的方法  #使用value属性获取当前单元格的值  content=bo1[\u0026#39;A4\u0026#39;].value content3=bo1.cell(row=4,column=2).value #也可以使用迭代 多个单元格在使用value方法，类似于list的切片  content4=bo1[\u0026#39;A1\u0026#39;:\u0026#39;C2\u0026#39;] #使用  3，写入,修改内容 3.1 针对单个单元格写入 参考代码\nimport openpyxl work=openpyxl.load_workbook(\u0026#34;dat.xlsx\u0026#34;) book=work.active print(\u0026#34;before:\u0026#34;,book.cell(1,1).value) book[\u0026#39;A1\u0026#39;].value=\u0026#34;1\u0026#34; #book.cell(1,1).value=\u0026#34;1\u0026#34; 效果等同于上调语句  print(\u0026#34;after:\u0026#34;,book.","title":"Python处理excel文件"},{"content":"安装 安装： pip install jupyter\n启动命令：jupyter notebook\n配置： 更换默认工作目录  cmd 键入 jupyter notebook --generate-config  打开配置文件，将 c.NotebookApp.notebook_dir 字段后的目录更改为自己的目录即可   参考   Windows下的Jupyter Notebook 安装与自定义启动（图文详解）\nnotebook使用virtualenv的虚拟环境   激活/注册 虚拟环境\n  安装 pip install ipykernel\n  将当前虚拟环境加入IPykernel\n  python -m ipykernel install --user --name=shiyanlou  -name后的字段为notebook中显示的名称，入下图\n启动notebook，在服务中切换   参考链接1  提高效率插件 多次强制输出 from IPython.core.interactiveshell import InteractiveShell InteractiveShell.ast_node_interactivity = \u0026#34;all\u0026#34; 自动补全   安装 pip install jupyter_contrib_nbextensions\n  配置 jupyter contrib nbextension install --user --skip-running-check，配置前需要关闭 =koobeton\n  启动notebook，勾选设置。\n   上面两个步骤都没问题上面两个步骤都没问题上面两个步骤都没问题上面两个步骤都没问,启动notebook\n点开 Nbextensions 的选项，并勾选 Hinterland\n参考链接\n修改字体   安装网页插件Stylus \n  为网页编辑脚本\n   - 添加样式：\n/* Markdown */ div#notebook { font-family: san francisco, \u0026#34;PingFangSC-Medium\u0026#34;, \u0026#34;Microsoft YaHei\u0026#34;; line-height: 20px; -webkit-font-smoothing: antialiased !important; } /* Markdown - h2 */ div#notebook h2 { color: #007aff; } /* Markdown - quote */ div#notebook blockquote { background-color: #f8f8f8; color: #505050; padding: 8.5px; margin: 0.5em -0.5em 0.5em -0.4em; } /* Markdown - code in paragraph */ div#notebook p code, div#notebook li code { font-family: Consolas, \u0026#34;PingFangSC-Medium\u0026#34;, \u0026#34;Microsoft YaHei\u0026#34;; font-size: 1em !important; color: #111111; border: 0.5px solid #cfcfcf; border-radius: 2px; background-color: #f7f7f7; padding: .1em .2em; margin: 0px 2px; } /* Markdown - code */ div.text_cell_render pre { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; padding: 8.5px; margin: 0.5em -0.5em 0.5em -0.4em; } div.text_cell_render code { background: #f7f7f7; } /* Code */ div.CodeMirror pre { font-family: Consolas, \u0026#34;PingFangSC-Medium\u0026#34;, \u0026#34;Microsoft YaHei\u0026#34;; font-size: 11pt; line-height: 140%; -webkit-font-smoothing: antialiased !important; } /* Code - output */ div.output pre { font-family: Consolas, \u0026#34;PingFangSC-Medium\u0026#34;, \u0026#34;Microsoft YaHei\u0026#34;; line-height: 20px; -webkit-font-smoothing: antialiased !important; } /* Code - comment */ span.cm-comment { font-family: san francisco, \u0026#34;PingFangSC-Medium\u0026#34;, \u0026#34;Microsoft YaHei\u0026#34; !important; font-style: normal !important; } matplotlib 输出高清大图 添加2，3行语句即可\nimport matplotlib.pyplot as plt %matplotlib inline %config InlineBackend.figure_format = \u0026#39;svg\u0026#39; ","permalink":"http://www.shiluan.space/post/jupyter%E9%85%8D%E7%BD%AE/","summary":"安装 安装： pip install jupyter\n启动命令：jupyter notebook\n配置： 更换默认工作目录  cmd 键入 jupyter notebook --generate-config  打开配置文件，将 c.NotebookApp.notebook_dir 字段后的目录更改为自己的目录即可   参考   Windows下的Jupyter Notebook 安装与自定义启动（图文详解）\nnotebook使用virtualenv的虚拟环境   激活/注册 虚拟环境\n  安装 pip install ipykernel\n  将当前虚拟环境加入IPykernel\n  python -m ipykernel install --user --name=shiyanlou  -name后的字段为notebook中显示的名称，入下图\n启动notebook，在服务中切换   参考链接1  提高效率插件 多次强制输出 from IPython.core.interactiveshell import InteractiveShell InteractiveShell.ast_node_interactivity = \u0026#34;all\u0026#34; 自动补全   安装 pip install jupyter_contrib_nbextensions","title":"jupyter notebook 安装、配置、优化"},{"content":" 1.异步编程介绍： 在并发执行的异步模型中，许多任务被穿插在同一时间线上，所有的任务都由一个控制流执行（单一线程）。任务的执行可能被暂停或恢复，中间的这段时间线程将会去执行其他任务。\n与多线程的区别 ：多线程由操作系统决定在时间线上什么时候挂起某个活动或恢复某个活动，而在异步并发模型中，程序员必须假设线程可能在任何时间被挂起和替换。\n2.使用Python的 concurrent.futures 模块 https://docs.python.org/zh-cn/3/library/concurrent.futures.html\n这个模块具有线程池和进程池、管理并行编程任务、处理非确定性的执行流程、进程/线程同步等功能\n 该模块由以下部分组成：   - concurrent.futures.Executor: 这是一个虚拟基类，提供了异步执行的方法。\n current.Futures 模块提供了两种 Executor 的子类，各自独立操作一个线程池和一个进程池。\n - concurrent.futures.ThreadPoolExecutor(max_workers)\n - concurrent.futures.ProcessPoolExecutor(max_workers)\n - submit(function, argument): 调度函数（可调用的对象）的执行，将 argument 作为参数传入。\n - map(function, argument): 将 argument 作为参数执行函数，以 异步 的方式。\n - shutdown(Wait=True): 发出让执行者释放所有资源的信号。\n - concurrent.futures.Future: 其中包括函数的异步执行。Future对象是submit任务（即带有参数的functions）到executor的实例。\n线程池和进程池 用池可以来简化线程/进程的使用。\n池包括两部分，一部分是内部的队列，存放着待执行的任务；另一部分是一系列的进程或线程，用于执行这些任务。\n池的概念主要目的是为了重用 ：让线程或进程在生命周期内可以多次使用。它减少了创建创建线程和进程的开销，提高了程序性能。\nTalk is cheap 定义了两个函数，做单纯的加减计算\ndef evaluate_item(x): # 计算总和，这里只是为了消耗时间 result_item = count(x) # 打印输入和输出结果 return result_item def count(number) : i=0 for i in range(0, 10000000): i=i+1 return i * number 通过三种方式执行对比运行时间，顺序执行和多线程执行的时间基本是一样的，而多进程的执行时间明显缩短。\nnumbers=[i for i in range(20)] # 顺序 start_time=time.time() for each in numbers: evaluate_item(each) print(\u0026#34;顺序执行：{}s \u0026#34;.format(time.time()-start_time)) #线程池 start_time=time.time() with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor: futures=[executor.submit(evaluate_item,each) for each in numbers] # for future in futures: # print(future.result()) print(\u0026#34;多线程执行：{}s \u0026#34;.format(time.time()-start_time)) start_time=time.time() with concurrent.futures.ProcessPoolExecutor(max_workers=8) as executor: futures_process=[executor.submit(evaluate_item,each) for each in numbers] # for future in futures: # print(future.result()) print(\u0026#34;多进程执行：{}s \u0026#34;.format(time.time()-start_time)) # 顺序执行：8.116609573364258 s # 多线程执行：8.308442831039429 s # 多进程执行：1.693899154663086 s  3. 使用Asyncio管理事件循环 Python的Asyncio模块提供了管理事件、协程、任务和线程的方法，以及编写并发代码的原语\n  事件循环 : 在Asyncio模块中，每一个进程都有一个事件循环。\n  协程 : 这是子程序的泛化概念。协程可以在执行期间暂停，这样就可以等待外部的处理（例如IO）完成之后，从之前暂停的地方恢复执行。\n  Futures : 定义了 Future 对象，和 concurrent.futures 模块一样，表示尚未完成的计算。\n  Tasks : 这是Asyncio的子类，用于封装和管理 并行模式下的协程。\n  Asncio提供的主要方法   loop = get_event_loop(): 得到当前上下文的事件循环。\n  loop.call_later(time_delay, callback, argument): 延后 time_delay 秒再执行 callback 方法。\n  loop.call_soon(callback, argument): 尽可能快调用 callback, call_soon() 函数结束。即主线程回到事件循环之后就会马上调用 callback 。\n  loop.time(): 以float类型返回当前时间循环的内部时间。\n  asyncio.set_event_loop(): 为当前上下文设置事件循环。\n  asyncio.new_event_loop(): 根据此策略创建一个新的时间循环并返回。\n  loop.run_forever(): 在调用 stop() 之前将一直运行。\n  Talk is cheap 示例中使用了三个函数来循环调用，三个函数分别运行1s，然后调用下一个事件\n 函数定义  def function_1(end_time, loop): print (\u0026#34;function_1 called\u0026#34;) if (loop.time() + 1.0) \u0026lt; end_time: loop.call_later(1, function_2, end_time, loop) else: loop.stop() def function_2(end_time, loop): print (\u0026#34;function_2 called \u0026#34;) if (loop.time() + 1.0) \u0026lt; end_time: loop.call_later(1, function_3, end_time, loop) else: loop.stop() def function_3(end_time, loop): print (\u0026#34;function_3 called\u0026#34;) if (loop.time() + 1.0) \u0026lt; end_time: loop.call_later(1, function_1, end_time, loop) else: loop.stop() 循环调用\nloop = asyncio.get_event_loop() end_loop = loop.time() + 9.0 loop.call_soon(function_1, end_loop, loop) # end_loop, loop都是参数传递 loop.run_forever() loop.close() 4.使用Asyncio管理协程 在上面的循环事件管理例子中，就是将一个任务划分成三个子程序，每一分实现特定的功能。子程序不能单独执行，只能在主程序的请求下执行，主程序负责协调使用各个子程序。\n协程就是子程序的泛化。和子程序一样的事，协程只负责计算任务的一步。不同的是协程没有主程序来调用，而是通过管道连接在一起。协程的执行点可以被挂起，也可以从挂起的点恢复。\n 协程可以有多个入口点，并可以yield多次   yield表示协程在此暂停，并且将执行权交给其他协程。因为协程可以将值与控制权一起传递给另一个协程，所以“yield一个值”就表示将值传给下一个执行的协程。\n 协程可以将执行权交给其他协程  Talk is cheap  定义协程只需要为函数加上@asyncio.coroutine装饰器即可  本例使用Asyncio的协程模拟状态机\n状态机中主要由4个状态，分别通过不同的输入产生。\n@asyncio.coroutine def state1(value): output=\u0026#34;state1 receive : {}\u0026#34;.format(value) input_value=randint(0,1) print(\u0026#34;state1 evaluating ...\u0026#34;) time.sleep(1) if input_value: result = yield from state2(input_value) else: result = yield from state3(input_value) output+=\u0026#34;\\nstate1 callback : {}\u0026#34;.format(result) return output @asyncio.coroutine def state2(value): output=\u0026#34;state2 receive : {}\u0026#34;.format(value) input_value=randint(0,1) print(\u0026#34;state1 evaluating ...\u0026#34;) time.sleep(1) if input_value: result = yield from state3(input_value) else: result = yield from state1(input_value) output+=\u0026#34;\\nstate2 callback : {}\u0026#34;.format(result) return output @asyncio.coroutine def state3(value): output=\u0026#34;state3 receive : {}\u0026#34;.format(value) input_value=randint(0,1) print(\u0026#34;state3 evaluating ...\u0026#34;) time.sleep(1) if input_value: result = yield from state4(input_value) else: result = yield from state1(input_value) output+=\u0026#34;\\nstate3 callback : {}\u0026#34;.format(result) return output @asyncio.coroutine def state4(value): ouput=\u0026#34;state4 receive : {}\u0026#34;.format(value) return ouput @asyncio.coroutine def create_state(): print(\u0026#34;start create state\\n\u0026#34;) input_value=randint(0,1) if input_value: result = yield from state4(input_value) else: result = yield from state1(input_value) print(\u0026#34;start state callback : \\n\u0026#34;,result) if __name__ == \u0026#39;__main__\u0026#39;: loop=asyncio.get_event_loop() loop.run_until_complete(create_state()) 参考链接 https://python-parallel-programmning-cookbook.readthedocs.io/zh_CN/latest/chapter4/01_Introduction.html\npython-parallel-programmning-cookbook\n","permalink":"http://www.shiluan.space/post/python%E5%BC%82%E6%AD%A5%E7%BC%96%E7%A8%8B/","summary":"1.异步编程介绍： 在并发执行的异步模型中，许多任务被穿插在同一时间线上，所有的任务都由一个控制流执行（单一线程）。任务的执行可能被暂停或恢复，中间的这段时间线程将会去执行其他任务。\n与多线程的区别 ：多线程由操作系统决定在时间线上什么时候挂起某个活动或恢复某个活动，而在异步并发模型中，程序员必须假设线程可能在任何时间被挂起和替换。\n2.使用Python的 concurrent.futures 模块 https://docs.python.org/zh-cn/3/library/concurrent.futures.html\n这个模块具有线程池和进程池、管理并行编程任务、处理非确定性的执行流程、进程/线程同步等功能\n 该模块由以下部分组成：   - concurrent.futures.Executor: 这是一个虚拟基类，提供了异步执行的方法。\n current.Futures 模块提供了两种 Executor 的子类，各自独立操作一个线程池和一个进程池。\n - concurrent.futures.ThreadPoolExecutor(max_workers)\n - concurrent.futures.ProcessPoolExecutor(max_workers)\n - submit(function, argument): 调度函数（可调用的对象）的执行，将 argument 作为参数传入。\n - map(function, argument): 将 argument 作为参数执行函数，以 异步 的方式。\n - shutdown(Wait=True): 发出让执行者释放所有资源的信号。\n - concurrent.futures.Future: 其中包括函数的异步执行。Future对象是submit任务（即带有参数的functions）到executor的实例。\n线程池和进程池 用池可以来简化线程/进程的使用。\n池包括两部分，一部分是内部的队列，存放着待执行的任务；另一部分是一系列的进程或线程，用于执行这些任务。\n池的概念主要目的是为了重用 ：让线程或进程在生命周期内可以多次使用。它减少了创建创建线程和进程的开销，提高了程序性能。\nTalk is cheap 定义了两个函数，做单纯的加减计算\ndef evaluate_item(x): # 计算总和，这里只是为了消耗时间 result_item = count(x) # 打印输入和输出结果 return result_item def count(number) : i=0 for i in range(0, 10000000): i=i+1 return i * number 通过三种方式执行对比运行时间，顺序执行和多线程执行的时间基本是一样的，而多进程的执行时间明显缩短。","title":"Python异步编程"},{"content":"需求： 视频的知识密度太高，一时间难以理解，最好的办法就是保留好这个视频的字幕截图，在自动拼接好。\n实现思路：  鼠标事件获取鼠标按下和释放的位置 注册快捷键，每按一次就对固定区域截图 图片拼接  实现   鼠标事件获取鼠标按下和释放的位置\n本来是被这个join事件困惑住，原来直接在子事件里stop就好了。\n  def on_click(x, y, button, pressed): global press_pos, flag global release_pos print(\u0026#39;{0}at {1}\u0026#39;.format(\u0026#39;Pressed\u0026#39; if pressed else \u0026#39;Released\u0026#39;, (x, y))) if pressed: press_pos = (x, y) else: release_pos = (x, y) if release_pos[0]-press_pos[0]\u0026gt;30: listener.stop() flag = False press_pos = () release_pos = () file=[] # Collect events until released with mouse.Listener( # on_move=on_move, on_click=on_click, on_scroll=on_scroll) as listener: listener.join() print(\u0026#34;截图位置获取成功\u0026#34;)   注册快捷键，每按一次就对固定区域截图\n没有把单张图片保存，直接读到内存里。f2为截图的快捷键，f4为停止截图的快捷键\n  def screen_it(): global file img = ImageGrab.grab(bbox=(press_pos[0], press_pos[1], release_pos[0], release_pos[1])) file.append(img) print(\u0026#34;已经截图\u0026#34;) file=[] keyboard.add_hotkey(\u0026#39;f2\u0026#39;,screen_it) keyboard.wait(\u0026#39;f4\u0026#39;)   图片拼接\n拼接就是用PIL中的拼接，文件保存后调用os.startfile()就可以用系统默认应用程序打开文件了~\n  def concat_img(file): width,height=file[0].size result = Image.new(file[0].mode,(width,height*len(file))) for i,im in enumerate(file): result.paste(im,box=(0,i*height)) file_name=\u0026#39;result{}.jpg\u0026#39;.format(time.strftime(\u0026#39;%H-%M-%S\u0026#39;, time.localtime())) result.save(file_name) return file_name fp=concat_img(file) print(\u0026#34;图片保存成功\u0026#34;) os.startfile(fp)  完整源码V1.0 2020/12/14  # -*- coding:utf-8 -*- # __author__ = \u0026#34;shiluanzzz\u0026#34; from pynput import mouse import keyboard import pyautogui import time from PIL import ImageGrab,Image import os def on_move(x, y): print(\u0026#39;Pointer moved to {0}\u0026#39;.format( (x, y))) def on_click(x, y, button, pressed): global press_pos, flag global release_pos print(\u0026#39;{0}at {1}\u0026#39;.format(\u0026#39;Pressed\u0026#39; if pressed else \u0026#39;Released\u0026#39;, (x, y))) if pressed: press_pos = (x, y) else: release_pos = (x, y) if release_pos[0]-press_pos[0]\u0026gt;30: listener.stop() def on_scroll(x, y, dx, dy): print(\u0026#39;Scrolled {0}at {1}\u0026#39;.format( \u0026#39;down\u0026#39; if dy \u0026lt; 0 else \u0026#39;up\u0026#39;, (x, y))) def screen_it(): global file img = ImageGrab.grab(bbox=(press_pos[0], press_pos[1], release_pos[0], release_pos[1])) file.append(img) print(\u0026#34;已经截图\u0026#34;) def concat_img(file): width,height=file[0].size result = Image.new(file[0].mode,(width,height*len(file))) for i,im in enumerate(file): result.paste(im,box=(0,i*height)) file_name=\u0026#39;result{}.jpg\u0026#39;.format(time.strftime(\u0026#39;%H-%M-%S\u0026#39;, time.localtime())) result.save(file_name) return file_name if __name__ == \u0026#39;__main__\u0026#39;: flag = False press_pos = () release_pos = () file=[] # Collect events until released with mouse.Listener( # on_move=on_move, on_click=on_click, on_scroll=on_scroll) as listener: listener.join() print(\u0026#34;截图位置获取成功\u0026#34;) keyboard.add_hotkey(\u0026#39;f2\u0026#39;,screen_it) keyboard.wait(\u0026#39;f4\u0026#39;) fp=concat_img(file) print(\u0026#34;图片保存成功\u0026#34;) os.startfile(fp) 参考链接：   https://blog.csdn.net/zhouchen1998/article/details/82080664\n  https://www.pythonf.cn/read/121179\n  ","permalink":"http://www.shiluan.space/post/%E8%87%AA%E5%8A%A8%E6%88%AA%E5%9B%BE%E5%B7%A5%E5%85%B7/","summary":"需求： 视频的知识密度太高，一时间难以理解，最好的办法就是保留好这个视频的字幕截图，在自动拼接好。\n实现思路：  鼠标事件获取鼠标按下和释放的位置 注册快捷键，每按一次就对固定区域截图 图片拼接  实现   鼠标事件获取鼠标按下和释放的位置\n本来是被这个join事件困惑住，原来直接在子事件里stop就好了。\n  def on_click(x, y, button, pressed): global press_pos, flag global release_pos print(\u0026#39;{0}at {1}\u0026#39;.format(\u0026#39;Pressed\u0026#39; if pressed else \u0026#39;Released\u0026#39;, (x, y))) if pressed: press_pos = (x, y) else: release_pos = (x, y) if release_pos[0]-press_pos[0]\u0026gt;30: listener.stop() flag = False press_pos = () release_pos = () file=[] # Collect events until released with mouse.Listener( # on_move=on_move, on_click=on_click, on_scroll=on_scroll) as listener: listener.","title":"Python自动截图拼接工具"},{"content":"参考链接 ： 知乎 博客\nRF框架参数意义    n_estimators:对原始数据集进行有放回抽样生成的子数据集个数，即决策树的个数。若n_estimators太小容易欠拟合，太大不能显著的提升模型，所以n_estimators选择适中的数值，版本0.20的默认值是10,版本0.22的默认值是100。\n  bootstrap:是否对样本集进行有放回抽样来构建树，True表示是,默认值True。\n  oob_score:是否采用袋外样本来评估模型的好坏\n   因为在有放回取样构建子模型的过程中，会有一部分数据没有取到，开启oob_score后可以使用这部分没有用到的数据在评估模型。\nRF决策树参数含义   max_features:构建决策树最优模型时考虑的最大特征数。   默认是\u0026quot;auto\u0026quot;，表示最大特征数是N的平方根;“log2\u0026quot;表示最大特征数是$log_{2}N$;\u0026ldquo;sqrt\u0026quot;表示最大特征数是$\\sqrt{N}$。\n max_depth:决策树最大深度。   若等于None,表示决策树在构建最优模型的时候不会限制子树的深度。如果模型样本量多，特征也多的情况下，推荐限制最大深度 ；若样本量少或者特征少，则不限制最大深度。\n min_samples_leaf:叶子节点含有的最少样本。   若叶子节点样本数小于min_samples_leaf，则对该叶子节点和兄弟叶子节点进行剪枝，只留下该叶子节点的父节点。整数型表示个数，浮点型表示取大于等于（样本数 * min_samples_leaf)的最小整数。min_samples_leaf默认值是1。\n min_samples_split:节点可分的最小样本数，   默认值是2。整数型和浮点型的含义与min_samples_leaf类似。\n max_leaf_nodes:最大叶子节点数。   int设置节点数,None表示对叶子节点数没有限制。\n min_impurity_decrease:节点划分的最小不纯度。   假设不纯度用信息增益表示，若某节点划分时的信息增益大于等于min_impurity_decrease，那么该节点还可以再划分；反之，则不能划分。\n criterion:表示节点的划分标准。   不纯度标准参考Gini指数，信息增益标准参考\u0026quot;entrop\u0026quot;熵。\n min_samples_leaf:叶子节点最小的样本权重和。   叶子节点如果小于这个值，则会和兄弟节点一起被剪枝，只保留叶子节点的父节点。默认是0，则不考虑样本权重问题。一般来说，如果有较多样本的缺失值或偏差很大，则尝试设置该参数值。\n随机生成最佳参数 随机生成最佳参数，使用RandomizedSearchCV() 从数据表格中随机抽取参数配置，对比选择最好的。缩小范围后在进行进一步调参。\nX,y=read_data() random_grid={ \u0026#39;n_estimators\u0026#39;: [i for i in range(10,1000,10)], \u0026#39;bootstrap\u0026#39;: [True,False], \u0026#39;max_depth\u0026#39;: [i for i in range(1,20,1)], \u0026#39;max_features\u0026#39;: [\u0026#39;auto\u0026#39;], \u0026#39;min_samples_leaf\u0026#39;: [i for i in range(1,30,1)], \u0026#39;min_samples_split\u0026#39;: [i for i in range(10,200,10)], } rf=RandomForestRegressor() rf_search=RandomizedSearchCV(estimator=rf, param_distributions=random_grid, n_iter=50, scoring=\u0026#39;neg_mean_absolute_error\u0026#39;, cv=3,verbose=2,random_state=233, n_jobs=-1 ) rf_search.fit(X,y) print(rf_search.best_params_) print(rf_search.best_estimator_) print(rf_search.best_score_) 随机森林调优实例 1.使用默认参数进行训练 from sklearn.ensemble import RandomForestClassifier rf_clf2=RandomForestClassifier( oob_score=True, random_state=666 ) rf_clf2.fit(X,y) rf_clf2.oob_score_ 可以看到使用默认参数的预测结果为0.89\n# 输出结果 RandomForestClassifier(bootstrap=True, class_weight=None, criterion=\u0026#39;gini\u0026#39;, max_depth=None, max_features=\u0026#39;auto\u0026#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None, oob_score=True, random_state=666, verbose=0, warm_start=False) 0.8926808073626417 2.对n_estimators参数择优 只对n_estimators进行调整，其他参数默认。n_estimators的择优范围是100-200，步长为2。(范围根据实际情况调整，因电脑性能较好，步长设置小)\ncv: 交叉验证参数，默认None，使用三折交叉验证。指定fold数量，默认为3。\nfrom sklearn.model_selection import GridSearchCV,cross_validate param_test1={\u0026#39;n_estimators\u0026#39;:range(100,220,2)} gsearch1=GridSearchCV( estimator=RandomForestClassifier(min_samples_split=100, min_samples_leaf=20, max_depth=10, random_state=10, max_features=\u0026#39;sqrt\u0026#39;, n_jobs=-1, ), param_grid=param_test1,scoring=\u0026#39;roc_auc\u0026#39;,cv=5 ) gsearch1.fit(X,y) 训练结束后，使用 best_params_和best_estimator_ 查看最优参数和最优模型参数。\n3.对max_depth，min_samples_split调参 在调节出 n_estimators 的最优参数后，对max_depth和min_samples_split参数进行调整。\n训练结束后网格搜索参数如右图。\n%%time param_test2={\u0026#39;max_depth\u0026#39;:range(10,30,1), \u0026#39;min_samples_split\u0026#39;:range(2,10,2)} gsearch2=GridSearchCV( estimator=RandomForestClassifier( n_estimators=164, min_samples_leaf=20, random_state=10, max_features=\u0026#39;sqrt\u0026#39;, oob_score=True, n_jobs=-1, ), param_grid=param_test2,scoring=\u0026#39;roc_auc\u0026#39;,cv=5,iid=False,) gsearch2.fit(X,y) 4.对min_samples_leaf 调参 继续对min_samples_leaf进行调参\n根据结果可以查看最优参数和最优评分，如右图\nparam_test3={\u0026#39;min_samples_leaf\u0026#39;:range(1,20,1)} gsearch3=GridSearchCV( estimator=RandomForestClassifier( n_estimators=165, max_depth=19, random_state=10, max_features=\u0026#39;sqrt\u0026#39;, oob_score=True, min_samples_split=2, n_jobs=-1, ), param_grid=param_test3,scoring=\u0026#39;roc_auc\u0026#39;,cv=5,iid=False,) gsearch3.fit(X,y) 5.max_features调参 param_test4={\u0026#39;max_features\u0026#39;:range(2,14,1)} gsearch4=GridSearchCV( estimator=RandomForestClassifier( n_estimators=165, max_depth=13, random_state=10, min_samples_leaf=3, min_samples_split=2, oob_score=True, n_jobs=-1, ), param_grid=param_test4,scoring=\u0026#39;roc_auc\u0026#39;,cv=5,iid=False, ) gsearch4.fit(X,y) 6 调参结束， from sklearn.ensemble import RandomForestClassifier random_forest_model=RandomForestClassifier( n_estimators=164, max_depth=13, random_state=10, min_samples_leaf=3, min_samples_split=2, oob_score=True, n_jobs=-1, max_features=6, ) random_forest_model.fit(X,y) random_forest_model.oob_score_ ","permalink":"http://www.shiluan.space/post/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E8%B0%83%E5%8F%82/","summary":"参考链接 ： 知乎 博客\nRF框架参数意义    n_estimators:对原始数据集进行有放回抽样生成的子数据集个数，即决策树的个数。若n_estimators太小容易欠拟合，太大不能显著的提升模型，所以n_estimators选择适中的数值，版本0.20的默认值是10,版本0.22的默认值是100。\n  bootstrap:是否对样本集进行有放回抽样来构建树，True表示是,默认值True。\n  oob_score:是否采用袋外样本来评估模型的好坏\n   因为在有放回取样构建子模型的过程中，会有一部分数据没有取到，开启oob_score后可以使用这部分没有用到的数据在评估模型。\nRF决策树参数含义   max_features:构建决策树最优模型时考虑的最大特征数。   默认是\u0026quot;auto\u0026quot;，表示最大特征数是N的平方根;“log2\u0026quot;表示最大特征数是$log_{2}N$;\u0026ldquo;sqrt\u0026quot;表示最大特征数是$\\sqrt{N}$。\n max_depth:决策树最大深度。   若等于None,表示决策树在构建最优模型的时候不会限制子树的深度。如果模型样本量多，特征也多的情况下，推荐限制最大深度 ；若样本量少或者特征少，则不限制最大深度。\n min_samples_leaf:叶子节点含有的最少样本。   若叶子节点样本数小于min_samples_leaf，则对该叶子节点和兄弟叶子节点进行剪枝，只留下该叶子节点的父节点。整数型表示个数，浮点型表示取大于等于（样本数 * min_samples_leaf)的最小整数。min_samples_leaf默认值是1。\n min_samples_split:节点可分的最小样本数，   默认值是2。整数型和浮点型的含义与min_samples_leaf类似。\n max_leaf_nodes:最大叶子节点数。   int设置节点数,None表示对叶子节点数没有限制。\n min_impurity_decrease:节点划分的最小不纯度。   假设不纯度用信息增益表示，若某节点划分时的信息增益大于等于min_impurity_decrease，那么该节点还可以再划分；反之，则不能划分。\n criterion:表示节点的划分标准。   不纯度标准参考Gini指数，信息增益标准参考\u0026quot;entrop\u0026quot;熵。\n min_samples_leaf:叶子节点最小的样本权重和。   叶子节点如果小于这个值，则会和兄弟节点一起被剪枝，只保留叶子节点的父节点。默认是0，则不考虑样本权重问题。一般来说，如果有较多样本的缺失值或偏差很大，则尝试设置该参数值。\n随机生成最佳参数 随机生成最佳参数，使用RandomizedSearchCV() 从数据表格中随机抽取参数配置，对比选择最好的。缩小范围后在进行进一步调参。\nX,y=read_data() random_grid={ \u0026#39;n_estimators\u0026#39;: [i for i in range(10,1000,10)], \u0026#39;bootstrap\u0026#39;: [True,False], \u0026#39;max_depth\u0026#39;: [i for i in range(1,20,1)], \u0026#39;max_features\u0026#39;: [\u0026#39;auto\u0026#39;], \u0026#39;min_samples_leaf\u0026#39;: [i for i in range(1,30,1)], \u0026#39;min_samples_split\u0026#39;: [i for i in range(10,200,10)], } rf=RandomForestRegressor() rf_search=RandomizedSearchCV(estimator=rf, param_distributions=random_grid, n_iter=50, scoring=\u0026#39;neg_mean_absolute_error\u0026#39;, cv=3,verbose=2,random_state=233, n_jobs=-1 ) rf_search.","title":"随机森林调参"},{"content":"字符串映射规则 通过位置 str.format() 可以接受不限个参数，位置可以不按顺序:\n\u0026gt;\u0026gt;\u0026gt; \u0026#34;{0}{1}\u0026#34;.format(\u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;) \u0026#39;hello world\u0026#39; \u0026gt;\u0026gt;\u0026gt; \u0026#34;{}{}\u0026#34;.format(\u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;) \u0026#39;hello world\u0026#39; \u0026gt;\u0026gt;\u0026gt; \u0026#34;{1}{0}{1}\u0026#34;.format(\u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;) \u0026#39;world hello world\u0026#39; 通过关键字参数 使用关键参数时字符串中需要提供参数名：\n\u0026gt;\u0026gt;\u0026gt; \u0026#34;I am {name}, age is {age}\u0026#34;.format(name=\u0026#34;huoty\u0026#34;, age=18) \u0026#39;I am huoty, age is 18\u0026#39; \u0026gt;\u0026gt;\u0026gt; user = {\u0026#34;name\u0026#34;: \u0026#34;huoty\u0026#34;, \u0026#34;age\u0026#34;: 18} \u0026gt;\u0026gt;\u0026gt; \u0026#34;I am {name}, age is {age}\u0026#34;.format(**user) \u0026#39;I am huoty, age is 18\u0026#39; 通过对象属性 str.format() 可以直接读取用户属性:\n\u0026gt;\u0026gt;\u0026gt; class User(object): ... def __init__(self, name, age): ... self.name = name ... self.age = age ... ... def __str__(self): ... return \u0026#34;{self.name}({self.age})\u0026#34;.format(self=self) ... ... def __repr__(self): ... return self.__str__() ... ... \u0026gt;\u0026gt;\u0026gt; user = User(\u0026#34;huoty\u0026#34;, 18) \u0026gt;\u0026gt;\u0026gt; user huoty(18) \u0026gt;\u0026gt;\u0026gt; \u0026#34;I am {user.name}, age is {user.age}\u0026#34;.format(user=user) \u0026#39;I am huoty, age is 18\u0026#39; 通过下标 在需要格式化的字符串内部可以通过下标来访问元素：\n\u0026gt;\u0026gt;\u0026gt; names, ages = [\u0026#34;huoty\u0026#34;, \u0026#34;esenich\u0026#34;, \u0026#34;anan\u0026#34;], [18, 16, 8] \u0026gt;\u0026gt;\u0026gt; \u0026#34;I am {0[0]}, age is {1[2]}\u0026#34;.format(names, ages) \u0026#39;I am huoty, age is 8\u0026#39; \u0026gt;\u0026gt;\u0026gt; users = {\u0026#34;names\u0026#34;: [\u0026#34;huoty\u0026#34;, \u0026#34;esenich\u0026#34;, \u0026#34;anan\u0026#34;], \u0026#34;ages\u0026#34;: [18, 16, 8]} \u0026gt;\u0026gt;\u0026gt; \u0026#34;I am {names[0]}, age is {ages[0]}\u0026#34;.format(**users) 指定转化 可以指定字符串的转化类型：\nconversion ::= \u0026quot;r\u0026quot; | \u0026quot;s\u0026quot; | \u0026quot;a\u0026quot; 其中 \u0026ldquo;!r\u0026rdquo; 对应 repr()； \u0026ldquo;!s\u0026rdquo; 对应 str(); \u0026ldquo;!a\u0026rdquo; 对应 ascii()。 示例：\n\u0026gt;\u0026gt;\u0026gt; \u0026#34;repr() shows quotes: {!r}; str() doesn\u0026#39;t: {!s}\u0026#34;.format(\u0026#39;test1\u0026#39;, \u0026#39;test2\u0026#39;) \u0026#34;repr() shows quotes: \u0026#39;test1\u0026#39;; str() doesn\u0026#39;t: test2\u0026#34; 格式限定符 填充与对齐 填充常跟对齐一起使用。^, \u0026lt;,\u0026gt; 分别是居中、左对齐、右对齐，后面带宽度， : 号后面带填充的字符，只能是一个字符，不指定则默认是用空格填充。\n\u0026gt;\u0026gt;\u0026gt; \u0026#34;{:\u0026gt;8}\u0026#34;.format(\u0026#34;181716\u0026#34;) \u0026#39; 181716\u0026#39; \u0026gt;\u0026gt;\u0026gt; \u0026#34;{:0\u0026gt;8}\u0026#34;.format(\u0026#34;181716\u0026#34;) \u0026#39;00181716\u0026#39; \u0026gt;\u0026gt;\u0026gt; \u0026#34;{:-\u0026gt;8}\u0026#34;.format(\u0026#34;181716\u0026#34;) \u0026#39;--181716\u0026#39; \u0026gt;\u0026gt;\u0026gt; \u0026#34;{:-\u0026lt;8}\u0026#34;.format(\u0026#34;181716\u0026#34;) \u0026#39;181716--\u0026#39; \u0026gt;\u0026gt;\u0026gt; \u0026#34;{:-^8}\u0026#34;.format(\u0026#34;181716\u0026#34;) \u0026#39;-181716-\u0026#39; \u0026gt;\u0026gt;\u0026gt; \u0026#34;{:-\u0026lt;25}\u0026gt;\u0026#34;.format(\u0026#34;Here \u0026#34;) \u0026#39;Here --------------------\u0026gt;\u0026#39; 浮点精度 用 f 表示浮点类型，并可以在其前边加上精度控制：\n\u0026gt;\u0026gt;\u0026gt; \u0026#34;[ {:.2f}]\u0026#34;.format(321.33345) \u0026#39;[ 321.33 ]\u0026#39; \u0026gt;\u0026gt;\u0026gt; \u0026#34;[ {:.1f}]\u0026#34;.format(321.33345) \u0026#39;[ 321.3 ]\u0026#39; \u0026gt;\u0026gt;\u0026gt; \u0026#34;[ {:.4f}]\u0026#34;.format(321.33345) \u0026#39;[ 321.3335 ]\u0026#39; \u0026gt;\u0026gt;\u0026gt; \u0026#34;[ {:.4f}]\u0026#34;.format(321) \u0026#39;[ 321.0000 ]\u0026#39; 还可以为浮点数指定符号，+ 表示在正数前显示 +，负数前显示 -； （空格）表示在正数前加空格，在幅负数前加 -；- 与什么都不加（{:f}）时一致：\n\u0026gt;\u0026gt;\u0026gt; \u0026#39;{:+f}; {:+f}\u0026#39;.format(3.141592657, -3.141592657) \u0026#39;+3.141593; -3.141593\u0026#39; \u0026gt;\u0026gt;\u0026gt; \u0026#39;{: f}; {: f}\u0026#39;.format(3.141592657, -3.141592657) \u0026#39; 3.141593; -3.141593\u0026#39; \u0026gt;\u0026gt;\u0026gt; \u0026#39;{:f}; {:f}\u0026#39;.format(3.141592657, -3.141592657) \u0026#39;3.141593; -3.141593\u0026#39; \u0026gt;\u0026gt;\u0026gt; \u0026#39;{:-f}; {:-f}\u0026#39;.format(3.141592657, -3.141592657) \u0026#39;3.141593; -3.141593\u0026#39; \u0026gt;\u0026gt;\u0026gt; \u0026#39;{:+.4f}; {:+.4f}\u0026#39;.format(3.141592657, -3.141592657) \u0026#39;+3.1416; -3.1416\u0026#39; 截取字符串 截取字符串类似于浮点精度控制，只是不需要加 f 在指定是浮点数：\n\u0026gt;\u0026gt;\u0026gt; \u0026#39;{:.2}\u0026#39;.format(\u0026#34;hello\u0026#34;) \u0026#39;he\u0026#39; \u0026gt;\u0026gt;\u0026gt; \u0026#39;{:.3}\u0026#39;.format(\u0026#34;huayong\u0026#34;) \u0026#39;hua\u0026#39; 指定进制 \u0026gt;\u0026gt;\u0026gt; \u0026#34;int: {0:d}; hex: {0:x}; oct: {0:o}; bin: {0:b}\u0026#34;.format(18) \u0026#39;int: 18; hex: 12; oct: 22; bin: 10010\u0026#39; \u0026gt;\u0026gt;\u0026gt; \u0026#34;int: {0:d}; hex: {0:#x}; oct: {0:#o}; bin: {0:#b}\u0026#34;.format(18) \u0026#39;int: 18; hex: 0x12; oct: 0o22; bin: 0b10010\u0026#39; 千位分隔符 可以使用 \u0026ldquo;,\u0026rdquo; 来作为千位分隔符：\n\u0026gt;\u0026gt;\u0026gt; \u0026#39;{:,}\u0026#39;.format(1234567890) \u0026#39;1,234,567,890\u0026#39; 百分数显示 \u0026gt;\u0026gt;\u0026gt; \u0026#34;progress: {:.2%}\u0026#34;.format(19.88/22) \u0026#39;progress: 90.36%\u0026#39; 事实上，format 还支持更多的类型符号：\ntype ::= \u0026quot;b\u0026quot; | \u0026quot;c\u0026quot; | \u0026quot;d\u0026quot; | \u0026quot;e\u0026quot; | \u0026quot;E\u0026quot; | \u0026quot;f\u0026quot; | \u0026quot;F\u0026quot; | \u0026quot;g\u0026quot; | \u0026quot;G\u0026quot; | \u0026quot;n\u0026quot; | \u0026quot;o\u0026quot; | \u0026quot;s\u0026quot; | \u0026quot;x\u0026quot; | \u0026quot;X\u0026quot; | \u0026quot;%\u0026quot; 其他技巧 占位符嵌套 某些时候占位符嵌套还是很有用的：\n\u0026gt;\u0026gt;\u0026gt; \u0026#39;{0:{fill}{align}16}\u0026#39;.format(\u0026#34;hello\u0026#34;, fill=\u0026#39;*\u0026#39;, align=\u0026#39;^\u0026#39;) \u0026#39;*****hello******\u0026#39; \u0026gt;\u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; for num in range(5,12): ... for base in \u0026#34;dXob\u0026#34;: ... print(\u0026#34;{0:{width}{base}}\u0026#34;.format(num, base=base, width=5), end=\u0026#39; \u0026#39;) ... print() ... ... 5 5 5 101 6 6 6 110 7 7 7 111 8 8 10 1000 9 9 11 1001 10 A 12 1010 11 B 13 1011 作为函数使用 可以先不指定格式化参数，而是在不要的地方作为函数来调用：\n\u0026gt;\u0026gt;\u0026gt; email_f = \u0026#34;Your email address was {email}\u0026#34;.format \u0026gt;\u0026gt;\u0026gt; print(email_f(email=\u0026#34;suodhuoty@gmail.com\u0026#34;)) Your email address was sudohuoty@gmail.com 转义大括号 当在字符串中需要使用大括号时可以用大括号转义：\n\u0026gt;\u0026gt;\u0026gt; \u0026#34; The {}set is often represented as { {0}} \u0026#34;.format(\u0026#34;empty\u0026#34;) \u0026#39; The empty set is often represented as {0}\u0026#39; ","permalink":"http://www.shiluan.space/post/python%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%A0%BC%E5%BC%8F%E5%8C%96%E6%80%BB%E7%BB%93/","summary":"字符串映射规则 通过位置 str.format() 可以接受不限个参数，位置可以不按顺序:\n\u0026gt;\u0026gt;\u0026gt; \u0026#34;{0}{1}\u0026#34;.format(\u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;) \u0026#39;hello world\u0026#39; \u0026gt;\u0026gt;\u0026gt; \u0026#34;{}{}\u0026#34;.format(\u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;) \u0026#39;hello world\u0026#39; \u0026gt;\u0026gt;\u0026gt; \u0026#34;{1}{0}{1}\u0026#34;.format(\u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;) \u0026#39;world hello world\u0026#39; 通过关键字参数 使用关键参数时字符串中需要提供参数名：\n\u0026gt;\u0026gt;\u0026gt; \u0026#34;I am {name}, age is {age}\u0026#34;.format(name=\u0026#34;huoty\u0026#34;, age=18) \u0026#39;I am huoty, age is 18\u0026#39; \u0026gt;\u0026gt;\u0026gt; user = {\u0026#34;name\u0026#34;: \u0026#34;huoty\u0026#34;, \u0026#34;age\u0026#34;: 18} \u0026gt;\u0026gt;\u0026gt; \u0026#34;I am {name}, age is {age}\u0026#34;.format(**user) \u0026#39;I am huoty, age is 18\u0026#39; 通过对象属性 str.format() 可以直接读取用户属性:\n\u0026gt;\u0026gt;\u0026gt; class User(object): ... def __init__(self, name, age): ... self.name = name .","title":"Python字符串格式化总结"},{"content":"常用命令：   导出win虚拟环境中的第三方库 pip freeze \u0026gt; requirements.txt \n  Linux中指定Python版本 mkvirtualenv venv -p /usr/bin/python3\n  后台运行程序 nohup python -u wechat_tixing.py \u0026gt; nohup.out 2\u0026gt;\u0026amp;1 \u0026amp;c\n  gunicorn -w 1 -b 127.0.0.1:1234 app:app\n  获取gunicorn的进程树，然后kill掉顶层任务 pstree -ap|grep gunicorn\n  部署  编辑配置文件 /etc/nginx/conf.d/shitou.conf  新端口的话记得去控制台开放端口\n Nginx重启 ：  nginx -s reload service nginx restart  后端重启、停止  编辑配置文件 gunicorn.conf\n# 并行工作线程数 workers = 4 # 监听内网端口5000【按需要更改】 bind = \u0026#39;127.0.0.1:8000\u0026#39; # 设置守护进程【关闭连接时，程序仍在运行】 daemon = True # 设置超时时间120s，默认为30s。按自己的需求进行设置 timeout = 120 # 设置访问日志和错误信息日志路径 # accesslog = \u0026#39;./logs/acess.log\u0026#39; # errorlog = \u0026#39;./logs/error.log\u0026#39; 通过配置文件加载进程gunicorn app:app -c gunicorn.conf \n或者单独启动 gunicorn -w 4 -b 127.0.0.1:8000 app:app\n相关参考：  https://www.jianshu.com/p/7cc457e06aee  常见问题   Nginx内网可以访问，外网无法访问\n  检查防火墙是否放通\niptables -I INPUT -p tcp --dport 80 -j ACCEPT   检查云服务器运营商处是否放通端口\n    ","permalink":"http://www.shiluan.space/post/flask%E9%83%A8%E7%BD%B2/","summary":"常用命令：   导出win虚拟环境中的第三方库 pip freeze \u0026gt; requirements.txt \n  Linux中指定Python版本 mkvirtualenv venv -p /usr/bin/python3\n  后台运行程序 nohup python -u wechat_tixing.py \u0026gt; nohup.out 2\u0026gt;\u0026amp;1 \u0026amp;c\n  gunicorn -w 1 -b 127.0.0.1:1234 app:app\n  获取gunicorn的进程树，然后kill掉顶层任务 pstree -ap|grep gunicorn\n  部署  编辑配置文件 /etc/nginx/conf.d/shitou.conf  新端口的话记得去控制台开放端口\n Nginx重启 ：  nginx -s reload service nginx restart  后端重启、停止  编辑配置文件 gunicorn.conf\n# 并行工作线程数 workers = 4 # 监听内网端口5000【按需要更改】 bind = \u0026#39;127.","title":"Flask部署"},{"content":"tqdm为python进度条库，分为基于迭代的运行模式和手动更新的运行模式。\n基于迭代的tqdm进度条 from tqdm import tqdm import time 利用tqdm.tqdm，将for循环过程中进行迭代的对象简单包裹 for i in tqdm(range(10),desc=\u0026#39;进度条\u0026#39;): time.sleep(0.2) # 进度条: 100%|█████████████████████████████████████| 10/10 [00:02\u0026lt;00:00, 4.98it/s] 列表推导式 temp=[time.sleep(0.2) for i in tqdm(range(1,10))] # 100%|███████████████████████████████████████████████| 9/9 [00:01\u0026lt;00:00, 4.98it/s] 若只是使用在range中，提供了一个简化的版本 from tqdm import trange for i in trange(1,20,2): time.sleep(0.2) #100%|█████████████████████████████████████████████| 10/10 [00:02\u0026lt;00:00, 4.99it/s] 为每一次遍历都更新描述文字 bar=tqdm(range(10,20)) for i in bar: time.sleep(0.2) bar.set_description(f\u0026#34;第{i}轮\u0026#34;) #第19轮: 100%|█████████████████████████████████████| 10/10 [00:0, 4.98it/s]  针对jupyter的美化版本进度条  tqdm对jupyter notebook和jupyter lab有着特殊的支持，且使用方法非常简单，只需要将原有的from tqdm import XXX的相应功能导入格式修改为from tqdm.notebook import XXX就可以了 pip install ipywidgets  from tqdm.notebook import trange # for i in trange(1,20): # time.sleep(0.2) # 好像不太行 手动更新tqdm进度条 import time from tqdm import tqdm with tqdm(total=200) as pbar: pbar.set_description(\u0026#39;Processing:\u0026#39;) # total表示总的项目, 循环的次数20*10(每次更新数目) = 200(total) for i in range(20): # 进行动作, 这里是过0.1s time.sleep(0.1) # 进行进度更新, 这里设置10个 pbar.update(10) # Processing:: 100%|██████████| 200/200 [00:02\u0026lt;00:00, 91.94it/s] 模块参数说明 class tqdm(object): \u0026#34;\u0026#34;\u0026#34; Decorate an iterable object, returning an iterator which acts exactly like the original iterable, but prints a dynamically updating progressbar every time a value is requested. \u0026#34;\u0026#34;\u0026#34; def __init__(self, iterable=None, desc=None, total=None, leave=False, file=sys.stderr, ncols=None, mininterval=0.1, maxinterval=10.0, miniters=None, ascii=None, disable=False, unit=\u0026#39;it\u0026#39;, unit_scale=False, dynamic_ncols=False, smoothing=0.3, nested=False, bar_format=None, initial=0, gui=False):  iterable: 可迭代的对象, 在手动更新时不需要进行设置 desc: 字符串, 左边进度条描述文字 total: 总的项目数 leave: bool值, 迭代完成后是否保留进度条 file: 输出指向位置, 默认是终端, 一般不需要设置 ncols: 调整进度条宽度, 默认是根据环境自动调节长度, 如果设置为0, 就没有进度条, 只有输出的信息 unit: 描述处理项目的文字, 默认是\u0026rsquo;it', 例如: 100 it/s, 处理照片的话设置为\u0026rsquo;img' ,则为 100 img/s unit_scale: 自动根据国际标准进行项目处理速度单位的换算, 例如 100000 it/s \u0026raquo; 100k it/s  ","permalink":"http://www.shiluan.space/post/tqdm/","summary":"tqdm为python进度条库，分为基于迭代的运行模式和手动更新的运行模式。\n基于迭代的tqdm进度条 from tqdm import tqdm import time 利用tqdm.tqdm，将for循环过程中进行迭代的对象简单包裹 for i in tqdm(range(10),desc=\u0026#39;进度条\u0026#39;): time.sleep(0.2) # 进度条: 100%|█████████████████████████████████████| 10/10 [00:02\u0026lt;00:00, 4.98it/s] 列表推导式 temp=[time.sleep(0.2) for i in tqdm(range(1,10))] # 100%|███████████████████████████████████████████████| 9/9 [00:01\u0026lt;00:00, 4.98it/s] 若只是使用在range中，提供了一个简化的版本 from tqdm import trange for i in trange(1,20,2): time.sleep(0.2) #100%|█████████████████████████████████████████████| 10/10 [00:02\u0026lt;00:00, 4.99it/s] 为每一次遍历都更新描述文字 bar=tqdm(range(10,20)) for i in bar: time.sleep(0.2) bar.set_description(f\u0026#34;第{i}轮\u0026#34;) #第19轮: 100%|█████████████████████████████████████| 10/10 [00:0, 4.98it/s]  针对jupyter的美化版本进度条  tqdm对jupyter notebook和jupyter lab有着特殊的支持，且使用方法非常简单，只需要将原有的from tqdm import XXX的相应功能导入格式修改为from tqdm.notebook import XXX就可以了 pip install ipywidgets  from tqdm.","title":"Python Tqdm进度条使用"},{"content":"Pandas笔记 最佳阅读体验请阅读笔记原文\n什么是Pandas？ 一个开源的 Python类库：用于数据分析、数据处理、数据可视化\n  高性能\n  容易使用的数据结构\n  容易使用的数据分析工具\n  很方便和其它类库一起使用\n  numpy：用于数学计算\n  scikit- learn：用于机器学习\n  Pandas 读取数据 Pandas 需要先读取表格类型的数据\n读取文件   读取csv文件 ratings = pd.read_csv(fpath)\n  读取txt文件 pvuv = pd.read_csv()\n  pvuv = pd.read_csv( fpath, sep=\u0026#34;\\t\u0026#34;, header=None, names=[\u0026#39;pdate\u0026#39;, \u0026#39;pv\u0026#39;, \u0026#39;uv\u0026#39;] # 指定列名 )   读取excel文件 pvuv = pd.read_excel(fpath)\n  读取 sql文件 mysql_page = pd.read_sql(\u0026quot;select * from crazyant_pvuv\u0026quot;, con=conn)\n  import pymysql conn = pymysql.connect( 参数省略 ) #创建链接 mysql_page = pd.read_sql(\u0026#34;select * from crazyant_pvuv\u0026#34;, con=conn) 查看数据  查看前几行数据 ratings.head()   查看数据的形状，返回(行数、列数) ratings.shape   查看列名列表 ratings.columns   查看索引列 ratings.index   查看每列的数据类型 ratings.dtypes  Pandas数据结构 Series Series是一种类似于一维数组的对象，它由一组数据（不同数据类型）以及一组与之相关的数据标签（即索引）组成。\n创建默认Series  创建 s1 = pd.Series([1,'a',5.2,7])   创建的数据没有传入索引，则为默认索引0,1,2,3\n 获取数据 s1.values  # output: array([1, \u0026#39;a\u0026#39;, 5.2, 7], dtype=object) 创建具有标签的Series   创建带索引的Series\n  s2.index\n  #output Index([\u0026#39;d\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;c\u0026#39;], dtype=\u0026#39;object\u0026#39;) 使用字典创建Series 将字典传入即可\nsdata={\u0026#39;Ohio\u0026#39;:35000,\u0026#39;Texas\u0026#39;:72000,\u0026#39;Oregon\u0026#39;:16000,\u0026#39;Utah\u0026#39;:5000} s3=pd.Series(sdata) s3 # output  Ohio 35000 Texas 72000 Oregon 16000 Utah 5000 dtype: int64 通过便签索引数据 DataFrame DataFrame是一个表格型的数据结构\n  每列可以是不同的值类型（数值、字符串、布尔值等）\n  既有行索引index,也有列索引columns\n  可以被看做由Series组成的字典\n  创建dataframe最常用的方法，使用pd.read_XXX读取纯文本文件、excel、mysql数据库\n使用字典创建dataframe data={ \u0026#39;state\u0026#39;:[\u0026#39;Ohio\u0026#39;,\u0026#39;Ohio\u0026#39;,\u0026#39;Ohio\u0026#39;,\u0026#39;Nevada\u0026#39;,\u0026#39;Nevada\u0026#39;], \u0026#39;year\u0026#39;:[2000,2001,2002,2001,2002], \u0026#39;pop\u0026#39;:[1.5,1.7,3.6,2.4,2.9] } df = pd.DataFrame(data) 从DataFrame中查询数据   如果只查询一行、一列，返回的是pd.Series\n  如果查询多行、多列，返回的是pd.DataFrame\n  查询一列，结果是一个pd.Series df[\u0026#39;year\u0026#39;] # outout 0 2000 1 2001 2 2002 3 2001 4 2002 Name: year, dtype: int64 type(df[\u0026#39;year\u0026#39;])# outputpandas.core.series.Series  \u0026amp;ensp\n3.2 查询多列，结果是一个pd.DataFrame df[[\u0026#39;year\u0026#39;, \u0026#39;pop\u0026#39;]] type(df.loc[1:3]) #output pandas.core.frame.DataFrame Pandas 查询数据 后面的数据查询示例为一个天气数据，其中将日期作为索引 df.set_index('ymd', inplace=True)\n使用单个label值查询数 # 得到单个值df.loc[\u0026#39;2018-01-03\u0026#39;, \u0026#39;bWendu\u0026#39;]#output2 # 得到一个Seriesdf.loc[\u0026#39;2018-01-03\u0026#39;, [\u0026#39;bWendu\u0026#39;, \u0026#39;yWendu\u0026#39;]]#outputbWendu 2yWendu -5Name: 2018-01-03, dtype: object 因为查询的是单个数据，所以返回的是series。第二个字段指定需要返回的字段，若不指定，使用：代替，则返回所有字段。\n使用值列表批量查询 # 得到Seriesdf.loc[[\u0026#39;2018-01-03\u0026#39;,\u0026#39;2018-01-04\u0026#39;,\u0026#39;2018-01-05\u0026#39;], \u0026#39;bWendu\u0026#39;]#outputymd2018-01-03 22018-01-04 02018-01-05 3Name: bWendu, dtype: int32 使用数值区间进行范围查询 # 行index按区间，前面设置了日期作为索引。df.loc[\u0026#39;2018-01-03\u0026#39;:\u0026#39;2018-01-05\u0026#39;, :] # 行和列都按区间查询df.loc[\u0026#39;2018-01-03\u0026#39;:\u0026#39;2018-01-05\u0026#39;, \u0026#39;bWendu\u0026#39;:\u0026#39;fengxiang\u0026#39;] 使用条件表达式查询   最低温度低于-10度的列表 df.loc[df[\u0026quot;yWendu\u0026quot;]\u0026lt;-10, :]\n  使用组合条件查询最高温度小于30度，并且最低温度大于15度，并且是晴天，并且天气为优的数据\n  df.loc[(df[\u0026#34;bWendu\u0026#34;]\u0026lt;=30) \u0026amp; (df[\u0026#34;yWendu\u0026#34;]\u0026gt;=15) \u0026amp; (df[\u0026#34;tianqi\u0026#34;]==\u0026#39;晴\u0026#39;) \u0026amp; (df[\u0026#34;aqiLevel\u0026#34;]==1), :]  使用函数查询   - 直接写lambda表达式 df.loc[lambda df : (df[\u0026quot;bWendu\u0026quot;]\u0026lt;=30) \u0026amp; (df[\u0026quot;yWendu\u0026quot;]\u0026gt;=15), :]\n - 编写函数查询\n# 编写自己的函数，查询9月份，空气质量好的数据def query_my_data(df): return df.index.str.startswith(\u0026#34;2018-09\u0026#34;) \u0026amp; (df[\u0026#34;aqiLevel\u0026#34;]==1) df.loc[query_my_data, :] 查询空值\u0026amp;\u0026amp;删除含有空值的列 match_data=match_data[match_data[\u0026#39;home_player_1\u0026#39;].notnull()] Pandas 新增数据列   直接赋值\n  df.apply方法\n  df.assign方法\n  按条件选择分组分别赋值f\n  直接赋值 字符串替换\n# 替换掉温度的后缀℃df.loc[:, \u0026#34;bWendu\u0026#34;] = df[\u0026#34;bWendu\u0026#34;].str.replace(\u0026#34;℃\u0026#34;, \u0026#34;\u0026#34;).astype(\u0026#39;int32\u0026#39;)df.loc[:, \u0026#34;yWendu\u0026#34;] = df[\u0026#34;yWendu\u0026#34;].str.replace(\u0026#34;℃\u0026#34;, \u0026#34;\u0026#34;).astype(\u0026#39;int32\u0026#39;) 数据加减\n# 注意，df[\u0026#34;bWendu\u0026#34;]其实是一个Series，后面的减法返回的是Seriesdf.loc[:, \u0026#34;wencha\u0026#34;] = df[\u0026#34;bWendu\u0026#34;] - df[\u0026#34;yWendu\u0026#34;] df.apply() DataFrame.apply(func, axis=0, broadcast=False, raw=False, reduce=None, args=(), **kwds)\n函数需要自己实现，函数的传入参数根据axis来定，比如axis = 1，就会把一行数据作为Series的数据结构传入给自己实现的函数中，我们在函数中实现对Series不同属性之间的计算，返回一个结果，则apply函数会自动遍历每一行DataFrame的数据，最后将所有结果组合成一个Series数据结构并返回。\ndef get_wendu_type(x): if x[\u0026#34;bWendu\u0026#34;] \u0026gt; 33: return \u0026#39;高温\u0026#39; if x[\u0026#34;yWendu\u0026#34;] \u0026lt; -10: return \u0026#39;低温\u0026#39; return \u0026#39;常温\u0026#39;# 注意需要设置axis==1，这是series的index是columnsdf.loc[:, \u0026#34;wendu_type\u0026#34;] = df.apply(get_wendu_type, axis=1) df.assign() # 可以同时添加多个新的列df.assign( yWendu_huashi = lambda x : x[\u0026#34;yWendu\u0026#34;] * 9 / 5 + 32, # 摄氏度转华氏度 bWendu_huashi = lambda x : x[\u0026#34;bWendu\u0026#34;] * 9 / 5 + 32) 按条件选择分组分别赋值 按条件先选择数据，然后对这部分数据赋值新列\n实例：高低温差大于10度，则认为温差大\n# 先创建空列（这是第一种创建新列的方法）df[\u0026#39;wencha_type\u0026#39;] = \u0026#39;\u0026#39;df.loc[df[\u0026#34;bWendu\u0026#34;]-df[\u0026#34;yWendu\u0026#34;]\u0026gt;10, \u0026#34;wencha_type\u0026#34;] = \u0026#34;温差大\u0026#34;df.loc[df[\u0026#34;bWendu\u0026#34;]-df[\u0026#34;yWendu\u0026#34;]\u0026lt;=10, \u0026#34;wencha_type\u0026#34;] = \u0026#34;温差正常\u0026#34; Pandas 数据统计函数   汇总类统计\n  唯一去重和按值计数\n  相关系数和协方差\n  汇总统计类   统计所有结果 df.describe()\n  单个数据的平均值df[\u0026quot;bWendu\u0026quot;].mean()\n  单个数据的最大值，最小值 df[\u0026quot;bWendu\u0026quot;].max()  df[\u0026quot;bWendu\u0026quot;].min()\n  唯一去重 一般用于枚举，分类列\ndf[\u0026#34;fengxiang\u0026#34;].unique()#outputarray([\u0026#39;东北风\u0026#39;, \u0026#39;北风\u0026#39;, \u0026#39;西北风\u0026#39;, \u0026#39;西南风\u0026#39;, \u0026#39;南风\u0026#39;, \u0026#39;东南风\u0026#39;, \u0026#39;东风\u0026#39;, \u0026#39;西风\u0026#39;], dtype=object)df[\u0026#34;fengli\u0026#34;].unique()#outputarray([\u0026#39;1-2级\u0026#39;, \u0026#39;4-5级\u0026#39;, \u0026#39;3-4级\u0026#39;, \u0026#39;2级\u0026#39;, \u0026#39;1级\u0026#39;, \u0026#39;3级\u0026#39;], dtype=object) 按值计数 df[\u0026quot;fengxiang\u0026quot;].value_counts()\n相关系数和协方差 用途：\n  两只股票，是不是同涨同跌？程度多大？正相关还是负相关？\n  产品销量的波动，跟哪些因素正相关、负相关，程度有多大？\n  来自知乎，对于两个变量X、Y：\n  协方差：衡量同向反向程度 ，如果协方差为正，说明X，Y同向变化，协方差越大说明同向程度越高；如果协方差为负，说明X，Y反向运动，协方差越小说明反向程度越高。\n  相关系数：衡量相似度程度 ，当他们的相关系数为1时，说明两个变量变化时的正向相似度最大，当相关系数为－1时，说明两个变量变化的反向相似度最大\n  编码\n 协方差矩阵： df.cov()   相关系数矩阵 df.corr()    单独查看两个指标的相关系数 df[\u0026quot;aqi\u0026quot;].corr(df[\u0026quot;bWendu\u0026quot;])\n  空气质量和温差的相关系数 df[\u0026quot;aqi\u0026quot;].corr(df[\u0026quot;bWendu\u0026quot;]-df[\u0026quot;yWendu\u0026quot;])\n  ","permalink":"http://www.shiluan.space/post/pandas%E6%93%8D%E4%BD%9C%E7%AC%94%E8%AE%B0/","summary":"Pandas笔记 最佳阅读体验请阅读笔记原文\n什么是Pandas？ 一个开源的 Python类库：用于数据分析、数据处理、数据可视化\n  高性能\n  容易使用的数据结构\n  容易使用的数据分析工具\n  很方便和其它类库一起使用\n  numpy：用于数学计算\n  scikit- learn：用于机器学习\n  Pandas 读取数据 Pandas 需要先读取表格类型的数据\n读取文件   读取csv文件 ratings = pd.read_csv(fpath)\n  读取txt文件 pvuv = pd.read_csv()\n  pvuv = pd.read_csv( fpath, sep=\u0026#34;\\t\u0026#34;, header=None, names=[\u0026#39;pdate\u0026#39;, \u0026#39;pv\u0026#39;, \u0026#39;uv\u0026#39;] # 指定列名 )   读取excel文件 pvuv = pd.read_excel(fpath)\n  读取 sql文件 mysql_page = pd.read_sql(\u0026quot;select * from crazyant_pvuv\u0026quot;, con=conn)","title":"Pandas学习笔记"},{"content":"paddle 手写数字识别 参考链接： 1 2\n任务介绍   手写识别属于典型的图像多分类问题\n  输入 \u0026amp;\u0026amp; 输出\n  MNIST数据集 美国国家标准与技术研究所 10个互斥类别，28 * 28的灰度图 250 个人的手写数字， 50% 来自人口普查局的 工作人员，50%来自高中学生 训练集：60000个样本 测试集：10000个样本\n数据准备  数据提供器   \u0026gt; batch reader: 用于读取数据的函数，数据可来自于文件、网络、随机数生成器等，函数每次返回一个batch大小的数据项。\n - paddle.reader.shuffle()表示每次缓存BUF_SIZE个数据项，并进行打乱\n - paddle.batch()表示每BATCH_SIZE组成一个batch\nBUF_SIZE=512 # 每次提取的数据量 BATCH_SIZE=128 #  train_reader=paddle.batch( paddle.reader.shuffle(paddle.dataset.mnist.train(),buf_size=BUF_SIZE), batch_size=BATCH_SIZE ) test_reader=paddle.batch( paddle.reader.shuffle(paddle.dataset.mnist.test(),buf_size=BUF_SIZE), batch_size=BATCH_SIZE ) train_data=paddle.dataset.mnist.train() 网络配置  网络模型   - 网络模型图\n \u0026gt; fluid.layers.fc 该OP将在神经网络中构建一个全连接层。\ndef multilayer_percetron(input): # multilayer-多层的 percetron-感知器 # 两个影藏层 激活函数为relu hiddle1=fluid.layers.fc(input=input,size=100,act=\u0026#39;relu\u0026#39;) hiddle2=fluid.layers.fc(input=hiddle1,size=100,act=\u0026#39;relu\u0026#39;) # 输出层，使用softmax激活 prediction=fluid.layers.fc(input=hiddle2,size=10,act=\u0026#39;softmax\u0026#39;) return prediction 损失函数   - fluid.layers.cross_entropy 该OP计算输入input和标签label间的交叉熵\ncost=fluid.layers.cross_entropy(input=predict,label=label) avg_cost=fluid.layers.mean(cost) # 对损失函数求平均值，方便后面反向传播使用 ## 准确率 acc=fluid.layers.accuracy(input=predict,label=\u0026#39;label\u0026#39;) 优化函数   使用内置的Adam方法，并指定学习率\noptimizer=fluid.optimizer.AdamOptimizer(learning_rate=0.001) opts=optimizer.minimize(avg_cost) 模型训练、评估 # 创建executor usr_cuda=False # 使用CPU训练 place=fluid.CUDAPlace(0) if usr_cuda else fluid.CPUPlace() test_program=fluid.default_main_program().clone(for_test=True) exe=fluid.Executor(place) exe.run(fluid.default_startup_program()) # 数据提供器： 转成一种特殊的数据结构，使其可以输入到Executor中。 feeder=fluid.DataFeeder(place=place,feed_list=[image,label]) 训练 %time EPOCH_NUM=2 model_save_dir=\u0026#34;Handwriting_recognition_V1.0\u0026#34; for pass_id in range(EPOCH_NUM): for betch_id,data in enumerate(train_reader()): train_cost,train_acc=exe.run( program=fluid.default_main_program(), feed=feeder.feed(data), fetch_list=[avg_cost,acc] ) all_train_iter+=BATCH_SIZE all_train_iters.append(all_train_iter) all_train_costs.append(train_cost[0]) all_train_accs.append(train_acc[0]) if betch_id % 200 == 0: print(\u0026#39;Pass:%d, Batch:%d, Cost:%0.5f, Accuracy:%0.5f\u0026#39; % (pass_id, betch_id, train_cost[0], train_acc[0])) # test test_accs=[] test_costs=[] for betch_id,data in enumerate(test_reader()): test_cost,test_acc=exe.run( program=test_program, feed=feeder.feed(data), fetch_list=[avg_cost,acc] ) test_accs.append(test_acc) test_costs.append(test_cost) test_cost=(sum(test_costs))/len(test_costs) test_acc=(sum(test_accs))/len(test_accs) print(\u0026#34;test:%d, cost:%0.5f,accuracy:%0.5f\u0026#34;%(pass_id,test_cost,test_acc)) #保存模型 # 如果保存路径不存在就创建 if not os.path.exists(model_save_dir): os.makedirs(model_save_dir) print (\u0026#39;save models to %s\u0026#39; % (model_save_dir)) fluid.io.save_inference_model(model_save_dir, #保存推理model的路径 [\u0026#39;image\u0026#39;], #推理（inference）需要 feed 的数据 [predict], #保存推理（inference）结果的 Variables exe) #executor 保存 inference model print(\u0026#39;训练模型保存完成！\u0026#39;) draw_train_process(\u0026#34;training\u0026#34;, all_train_iters, all_train_costs, all_train_accs, \u0026#34;trainning cost\u0026#34;, \u0026#34;trainning acc\u0026#34;) ","permalink":"http://www.shiluan.space/post/paddle%E5%AE%9E%E6%88%98%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB/","summary":"paddle 手写数字识别 参考链接： 1 2\n任务介绍   手写识别属于典型的图像多分类问题\n  输入 \u0026amp;\u0026amp; 输出\n  MNIST数据集 美国国家标准与技术研究所 10个互斥类别，28 * 28的灰度图 250 个人的手写数字， 50% 来自人口普查局的 工作人员，50%来自高中学生 训练集：60000个样本 测试集：10000个样本\n数据准备  数据提供器   \u0026gt; batch reader: 用于读取数据的函数，数据可来自于文件、网络、随机数生成器等，函数每次返回一个batch大小的数据项。\n - paddle.reader.shuffle()表示每次缓存BUF_SIZE个数据项，并进行打乱\n - paddle.batch()表示每BATCH_SIZE组成一个batch\nBUF_SIZE=512 # 每次提取的数据量 BATCH_SIZE=128 #  train_reader=paddle.batch( paddle.reader.shuffle(paddle.dataset.mnist.train(),buf_size=BUF_SIZE), batch_size=BATCH_SIZE ) test_reader=paddle.batch( paddle.reader.shuffle(paddle.dataset.mnist.test(),buf_size=BUF_SIZE), batch_size=BATCH_SIZE ) train_data=paddle.dataset.mnist.train() 网络配置  网络模型   - 网络模型图\n \u0026gt; fluid.layers.fc 该OP将在神经网络中构建一个全连接层。\ndef multilayer_percetron(input): # multilayer-多层的 percetron-感知器 # 两个影藏层 激活函数为relu hiddle1=fluid.","title":"Paddle实战手写数字识别"}]